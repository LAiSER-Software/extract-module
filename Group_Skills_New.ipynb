{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMhYsplHFbx9",
        "outputId": "2058e978-4bf1-49ad-c106-d8b78c85db49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.30.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 825, in _parseNoCache\n",
            "    ret_tokens = ParseResults(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/results.py\", line 159, in __init__\n",
            "    def __init__(\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 207, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1465, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.10/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1218, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 686, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 636, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 119, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 502, in __init__\n",
            "    self.stack = StackSummary.extract(\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 383, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 306, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 396, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 365, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 323, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfCW3teuE7oz"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import spacy\n",
        "import openai\n",
        "import string\n",
        "import psutil\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import en_core_web_lg\n",
        "from openai import OpenAI\n",
        "from datetime import datetime\n",
        "\n",
        "# Set up paths\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "base_path = \"/content/drive/MyDrive\"\n",
        "\n",
        "# Set up logging\n",
        "log_file = f\"{base_path}/performance_logs.log\"\n",
        "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "def log_performance(function_name, start_time, end_time):\n",
        "    \"\"\"Log the performance metrics for a function.\"\"\"\n",
        "    execution_time = end_time - start_time\n",
        "    process = psutil.Process()\n",
        "    cpu_percent = process.cpu_percent()\n",
        "    memory_info = process.memory_info()\n",
        "    memory_usage = memory_info.rss / (1024 ** 2)  # Convert to MB\n",
        "\n",
        "    log_message = (\n",
        "        f\"Function: {function_name}\\n\"\n",
        "        f\"Execution time: {execution_time:.2f} seconds\\n\"\n",
        "        f\"CPU usage: {cpu_percent:.2f}%\\n\"\n",
        "        f\"Memory usage: {memory_usage:.2f} MB\\n\"\n",
        "        \"-------------------------------\"\n",
        "    )\n",
        "    logging.info(log_message)\n",
        "\n",
        "\n",
        "# Load GloVe vectors\n",
        "nlp_glove = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Set up OpenAI API key\n",
        "# TODO: Remove this api key before committing\n",
        "openai.api_key = \"sk-proj-aXyKoTNJA8Cd4E7fP9s0T3BlbkFJKVQSd6folsAavHi3mQWi\"\n",
        "client = OpenAI(api_key=openai.api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N0kzJrnFFYr"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def convert_to_jsonl(df, output_file):\n",
        "    \"\"\"Convert a DataFrame to a JSONL file.\"\"\"\n",
        "    jsonl_list = [\n",
        "        {\"prompt\": row[\"RSD Name\"], \"completion\": row[\"Skill Statement\"]}\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "    with open(output_file, \"w\") as jsonl_file:\n",
        "        for jsonl_dict in jsonl_list:\n",
        "            jsonl_file.write(\n",
        "                f'{{\"prompt\": \"{jsonl_dict[\"prompt\"]}\", \"completion\": \"{jsonl_dict[\"completion\"]}\"}}\\n'\n",
        "            )\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Calculate embeddings for a given text.\"\"\"\n",
        "    start_time = time.time()\n",
        "    doc = nlp_glove(text)\n",
        "    if len(doc) == 0:\n",
        "        return np.zeros(300)  # Return zeros for empty texts\n",
        "    end_time = time.time()\n",
        "    log_performance(\"Get Embedding Perf:\", start_time, end_time)\n",
        "    return np.mean([word.vector for word in doc], axis=0)\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "def extract_skills_from_job_description(job_description, fine_tuned_model_id):\n",
        "    \"\"\"Extract skills from a job description using a fine-tuned model.\"\"\"\n",
        "    start_time = time.time()\n",
        "    response = client.completions.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        prompt=f\"\"\"Name all the skills present in the following job description in a single list.\n",
        "                Response should have only the skills, no other information or words.\n",
        "                Skills should be keywords, each being no more than 3 words.:\n",
        "                This is the Job Description:\n",
        "                {job_description}\n",
        "\n",
        "                Skills:\n",
        "                \"\"\",\n",
        "        max_tokens=75,\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    extracted_skills = response.choices[0].text.strip()\n",
        "    extracted_skills_set = set(\n",
        "        [word.lstrip(\"-\").strip() for word in extracted_skills.split(\"\\n\")]\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    log_performance(\"Extracted Skills perf for each query/job/course\", start_time, end_time)\n",
        "    return list(extracted_skills_set)\n",
        "\n",
        "def compare_skills_with_glove(extracted_skills_list, taxn_source, similarity_threshold=0.65):\n",
        "    \"\"\"Match extracted skills with OSN skills using GloVe embeddings.\"\"\"\n",
        "    start_time = time.time()\n",
        "    key_series = taxn_source[\"RSD Name\"]\n",
        "    skill_matches = []\n",
        "    matched_skills_set = set()\n",
        "\n",
        "    for extracted_skill in tqdm(extracted_skills_list):\n",
        "        if extracted_skill.strip():\n",
        "            extracted_embedding = get_embedding(extracted_skill)\n",
        "            best_match = None\n",
        "            best_similarity = 0.0\n",
        "\n",
        "            for key_skill in key_series:\n",
        "                key_embedding = get_embedding(key_skill)\n",
        "                similarity = cosine_similarity(extracted_embedding, key_embedding)\n",
        "\n",
        "                if (\n",
        "                    similarity >= similarity_threshold\n",
        "                    and key_skill not in matched_skills_set\n",
        "                ):\n",
        "                    best_similarity = similarity\n",
        "                    best_match = key_skill\n",
        "                    matched_skills_set.add(key_skill)\n",
        "\n",
        "            if best_match:\n",
        "                skill_matches.append(best_match)\n",
        "\n",
        "    end_time = time.time()\n",
        "    log_performance(\"compare_skills_with_glove Perf.\", start_time, end_time)\n",
        "    return skill_matches\n",
        "\n",
        "import multiprocessing as mp\n",
        "from functools import partial\n",
        "\n",
        "def match_skills_for_job(job_desc, fine_tuned_model_id, taxn_source, similarity_threshold=0.65):\n",
        "    \"\"\"Match skills for a single job description.\"\"\"\n",
        "    extracted_skills = extract_skills_from_job_description(job_desc, fine_tuned_model_id)\n",
        "    job_matches = compare_skills_with_glove(extracted_skills, taxn_source, similarity_threshold)\n",
        "    return job_matches\n",
        "\n",
        "def match_skills_for_job_df_parallel(jobs_df, fine_tuned_model_id, taxn_source, similarity_threshold=0.65, num_processes=None):\n",
        "    \"\"\"Match skills for each job in a DataFrame using parallel processing.\"\"\"\n",
        "    start_time = time.time()\n",
        "    if num_processes is None:\n",
        "        num_processes = mp.cpu_count()\n",
        "\n",
        "    pool = mp.Pool(processes=num_processes)\n",
        "    match_func = partial(match_skills_for_job, fine_tuned_model_id=fine_tuned_model_id, taxn_source=taxn_source, similarity_threshold=similarity_threshold)\n",
        "    matched_skills_list = pool.map(match_func, [job_row[\"job_desc\"] for _, job_row in jobs_df.iterrows()])\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    matched_skills_df = pd.DataFrame(\n",
        "        [{\"Job Number\": job_index + 1, \"Matched Skills\": job_matches}\n",
        "         for job_index, job_matches in enumerate(matched_skills_list)],\n",
        "        columns=[\"Job Number\", \"Matched Skills\"]\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    log_performance(\"match_skills_for_job_df_parallel perf.\", start_time, end_time)\n",
        "    return matched_skills_df\n",
        "\n",
        "def match_skills_for_job_df(jobs_df, fine_tuned_model_id, taxn_source, similarity_threshold=0.65):\n",
        "    \"\"\"Match skills for each job in a DataFrame.\"\"\"\n",
        "    start_time = time.time()\n",
        "    matched_skills_list = []\n",
        "\n",
        "    for job_index, job_row in tqdm(jobs_df.iterrows()):\n",
        "        extracted_skills = extract_skills_from_job_description(\n",
        "            job_row[\"job_desc\"], fine_tuned_model_id\n",
        "        )\n",
        "        job_matches = compare_skills_with_glove(extracted_skills, taxn_source, similarity_threshold)\n",
        "        job_data = {\n",
        "            \"Job Number\": job_index + 1,\n",
        "            \"Matched Skills\": job_matches,\n",
        "        }\n",
        "        matched_skills_list.append(job_data)\n",
        "\n",
        "    matched_skills_df = pd.DataFrame(matched_skills_list)\n",
        "    end_time = time.time()\n",
        "    log_performance(\"match_skills_for_job_df perf.\", start_time, end_time)\n",
        "    return matched_skills_df\n",
        "\n",
        "def find_common_skills(job_skills_df, min_matches=3):\n",
        "    \"\"\"Find common skills between jobs.\"\"\"\n",
        "    start_time = time.time()\n",
        "    common_skills_pairs = []\n",
        "\n",
        "    for job_index, job_row in tqdm(job_skills_df.iterrows()):\n",
        "        current_job_number = job_row[\"Job Number\"]\n",
        "        current_job_skills = set(job_row[\"Matched Skills\"])\n",
        "\n",
        "        for other_job_index, other_job_row in job_skills_df.iloc[job_index + 1:].iterrows():\n",
        "            other_job_number = other_job_row[\"Job Number\"]\n",
        "            other_job_skills = set(other_job_row[\"Matched Skills\"])\n",
        "            common_skills = current_job_skills.intersection(other_job_skills)\n",
        "            num_common_skills = len(common_skills)\n",
        "\n",
        "            if num_common_skills >= min_matches:\n",
        "                common_skills_pairs.append(\n",
        "                    (current_job_number, other_job_number, list(common_skills))\n",
        "                )\n",
        "\n",
        "    skills_common_df = pd.DataFrame(\n",
        "        common_skills_pairs, columns=[\"Job Number 1\", \"Job Number 2\", \"Common Skills\"]\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    log_performance(\"find_common_skills perf.\", start_time, end_time)\n",
        "    return skills_common_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEK6kajmFHHB"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import torch\n",
        "\n",
        "# Load GloVe vectors\n",
        "nlp_glove = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the GloVe vectors to GPU if available\n",
        "if device.type == \"cuda\":\n",
        "    glove_vectors = nlp_glove.vocab.vectors.data\n",
        "    glove_vectors_tensor = torch.tensor(glove_vectors, device=device)\n",
        "    nlp_glove_gpu = spacy.vocab.Vocab(vectors=spacy.vocab.Vectors(data=glove_vectors_tensor))\n",
        "    nlp_gpu = spacy.lang.en.English(vocab=nlp_glove_gpu)\n",
        "else:\n",
        "    nlp_gpu = nlp_glove\n",
        "# Update the get_embedding function to use GPU\n",
        "def get_embedding(text):\n",
        "    \"\"\"Calculate embeddings for a given text using GPU if available.\"\"\"\n",
        "    doc = nlp_glove(text)\n",
        "    if len(doc) == 0:\n",
        "        return torch.zeros(300, device=device)  # Return zeros for empty texts\n",
        "    return torch.mean(\n",
        "        torch.stack([torch.tensor(word.vector, device=device) for word in doc]), dim=0\n",
        "    )\n",
        "\n",
        "# Update the cosine_similarity function to use GPU\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Calculate cosine similarity between two vectors on GPU if available.\"\"\"\n",
        "    vec1 = vec1.to(device)\n",
        "    vec2 = vec2.to(device)\n",
        "    return torch.dot(vec1, vec2) / (vec1.norm() * vec2.norm())\n",
        "\n",
        "# Count common Skills\n",
        "def count_common_skills(common_skills_df, job_skills_df):\n",
        "    \"\"\"Count the number of jobs that share a set of common skills.\"\"\"\n",
        "    start_time = time.time()\n",
        "    count_skills = pd.DataFrame(columns=['Count', 'Jobs', 'Common Skills'])\n",
        "\n",
        "    for index, row in tqdm(common_skills_df.iterrows()):\n",
        "        common_skills_set = set(row['Common Skills'])\n",
        "        count = 0\n",
        "        job_numbers = []\n",
        "\n",
        "        for job_index, job_row in job_skills_df.iterrows():\n",
        "            matched_skills_set = set(job_row['Matched Skills'])\n",
        "\n",
        "            if common_skills_set.issubset(matched_skills_set):\n",
        "                count += 1\n",
        "                job_numbers.append(job_row['Job Number'])\n",
        "\n",
        "        count_skills = pd.concat([count_skills, pd.DataFrame({\n",
        "            'Count': [count],\n",
        "            'Jobs': [job_numbers],\n",
        "            'Common Skills': [row['Common Skills']]\n",
        "        })], ignore_index=True)\n",
        "    end_time = time.time()\n",
        "    log_performance(\"count_common_skills perf.\", start_time, end_time)\n",
        "    return count_skills"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPqqSEWgFJnf"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "osn_comp_df = pd.read_csv('https://raw.githubusercontent.com/phanindra-max/LAiSER-datasets/master/osn_comp.csv')\n",
        "osn_pub_df = pd.read_csv('https://raw.githubusercontent.com/phanindra-max/LAiSER-datasets/master/osn_public_rel.csv')\n",
        "jobs_df = pd.read_csv('https://raw.githubusercontent.com/phanindra-max/LAiSER-datasets/master/jobs_df.csv')\n",
        "\n",
        "# Convert OSN data to JSONL format\n",
        "# convert_to_jsonl(osn_comp_df, \"osn_comp.jsonl\")\n",
        "# convert_to_jsonl(osn_pub_df, \"osn_pub.jsonl\")\n",
        "\n",
        "# Fine-tuned model IDs\n",
        "fine_tuned_model_comp = \"ft:davinci-002:personal::8IIFVUbf\"\n",
        "\n",
        "# Extract and match skills\n",
        "job_skills_comp = match_skills_for_job_df_parallel(\n",
        "    jobs_df, fine_tuned_model_comp, osn_comp_df, similarity_threshold=0.65\n",
        ")\n",
        "job_skills_pub = match_skills_for_job_df_parallel(\n",
        "    jobs_df, fine_tuned_model_comp, osn_pub_df, similarity_threshold=0.65\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlD22czRUyYY"
      },
      "outputs": [],
      "source": [
        "# Find common skills\n",
        "common_skills_comp = find_common_skills(job_skills_comp)\n",
        "common_skills_pub = find_common_skills(job_skills_pub)\n",
        "\n",
        "# Count common skills\n",
        "count_skills_comp = count_common_skills(common_skills_comp, job_skills_comp)\n",
        "count_skills_pub = count_common_skills(common_skills_pub, job_skills_pub)\n",
        "\n",
        "# Save results\n",
        "job_skills_comp.to_csv(f\"{base_path}/job_skills_comp.csv\", index=False)\n",
        "job_skills_pub.to_csv(f\"{base_path}/job_skills_pub.csv\", index=False)\n",
        "count_skills_comp.to_csv(f\"{base_path}/count_skills_comp.csv\", index=False)\n",
        "count_skills_pub.to_csv(f\"{base_path}/count_skills_pub.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo230DCjf-Uk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}