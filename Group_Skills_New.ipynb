{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMhYsplHFbx9",
        "outputId": "868fc2e9-a3f9-40be-d425-494ada0ae040"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.30.3-py3-none-any.whl (320 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m256.0/320.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.3\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfCW3teuE7oz",
        "outputId": "08f55a4c-3c35-4e3a-817b-1e09bce60f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import openai\n",
        "import string\n",
        "import en_core_web_lg\n",
        "from openai import OpenAI\n",
        "\n",
        "# Load GloVe vectors\n",
        "nlp_glove = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Set up OpenAI API key\n",
        "openai.api_key = \"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\"\n",
        "client = OpenAI(api_key=openai.api_key)\n",
        "\n",
        "# Set up paths\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "base_path = \"/content/drive/MyDrive\"  # Adjust this as needed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def convert_to_jsonl(df, output_file):\n",
        "    \"\"\"Convert a DataFrame to a JSONL file.\"\"\"\n",
        "    jsonl_list = [\n",
        "        {\"prompt\": row[\"RSD Name\"], \"completion\": row[\"Skill Statement\"]}\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "    with open(output_file, \"w\") as jsonl_file:\n",
        "        for jsonl_dict in jsonl_list:\n",
        "            jsonl_file.write(\n",
        "                f'{{\"prompt\": \"{jsonl_dict[\"prompt\"]}\", \"completion\": \"{jsonl_dict[\"completion\"]}\"}}\\n'\n",
        "            )\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Calculate embeddings for a given text.\"\"\"\n",
        "    doc = nlp_glove(text)\n",
        "    if len(doc) == 0:\n",
        "        return np.zeros(300)  # Return zeros for empty texts\n",
        "    return np.mean([word.vector for word in doc], axis=0)\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "def extract_skills_from_job_description(job_description, fine_tuned_model_id):\n",
        "    \"\"\"Extract skills from a job description using a fine-tuned model.\"\"\"\n",
        "    response = client.completions.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        prompt=f\"\"\"Name all the skills present in the following job description in a single list.\n",
        "                Response should have only the skills, no other information or words.\n",
        "                Skills should be keywords, each being no more than 3 words.:\n",
        "                This is the Job Description:\n",
        "                {job_description}\n",
        "\n",
        "                Skills:\n",
        "                \"\"\",\n",
        "        max_tokens=75,\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    extracted_skills = response.choices[0].text.strip()\n",
        "    extracted_skills_set = set(\n",
        "        [word.lstrip(\"-\").strip() for word in extracted_skills.split(\"\\n\")]\n",
        "    )\n",
        "    return list(extracted_skills_set)\n",
        "\n",
        "def compare_skills_with_glove(extracted_skills_list, taxn_source, similarity_threshold=0.65):\n",
        "    \"\"\"Match extracted skills with OSN skills using GloVe embeddings.\"\"\"\n",
        "    key_series = taxn_source[\"RSD Name\"]\n",
        "    skill_matches = []\n",
        "    matched_skills_set = set()\n",
        "\n",
        "    for extracted_skill in tqdm(extracted_skills_list):\n",
        "        if extracted_skill.strip():\n",
        "            extracted_embedding = get_embedding(extracted_skill)\n",
        "            best_match = None\n",
        "            best_similarity = 0.0\n",
        "\n",
        "            for key_skill in key_series:\n",
        "                key_embedding = get_embedding(key_skill)\n",
        "                similarity = cosine_similarity(extracted_embedding, key_embedding)\n",
        "\n",
        "                if (\n",
        "                    similarity >= similarity_threshold\n",
        "                    and key_skill not in matched_skills_set\n",
        "                ):\n",
        "                    best_similarity = similarity\n",
        "                    best_match = key_skill\n",
        "                    matched_skills_set.add(key_skill)\n",
        "\n",
        "            if best_match:\n",
        "                skill_matches.append(best_match)\n",
        "\n",
        "    return skill_matches\n",
        "\n",
        "def match_skills_for_job_df(jobs_df, fine_tuned_model_id, taxn_source, similarity_threshold=0.65):\n",
        "    \"\"\"Match skills for each job in a DataFrame.\"\"\"\n",
        "    matched_skills_list = []\n",
        "\n",
        "    for job_index, job_row in tqdm(jobs_df.iterrows()):\n",
        "        extracted_skills = extract_skills_from_job_description(\n",
        "            job_row[\"job_desc\"], fine_tuned_model_id\n",
        "        )\n",
        "        job_matches = compare_skills_with_glove(extracted_skills, taxn_source, similarity_threshold)\n",
        "        job_data = {\n",
        "            \"Job Number\": job_index + 1,\n",
        "            \"Matched Skills\": job_matches,\n",
        "        }\n",
        "        matched_skills_list.append(job_data)\n",
        "\n",
        "    matched_skills_df = pd.DataFrame(matched_skills_list)\n",
        "    return matched_skills_df\n",
        "\n",
        "def find_common_skills(job_skills_df, min_matches=3):\n",
        "    \"\"\"Find common skills between jobs.\"\"\"\n",
        "    common_skills_pairs = []\n",
        "\n",
        "    for job_index, job_row in tqdm(job_skills_df.iterrows()):\n",
        "        current_job_number = job_row[\"Job Number\"]\n",
        "        current_job_skills = set(job_row[\"Matched Skills\"])\n",
        "\n",
        "        for other_job_index, other_job_row in job_skills_df.iloc[job_index + 1:].iterrows():\n",
        "            other_job_number = other_job_row[\"Job Number\"]\n",
        "            other_job_skills = set(other_job_row[\"Matched Skills\"])\n",
        "            common_skills = current_job_skills.intersection(other_job_skills)\n",
        "            num_common_skills = len(common_skills)\n",
        "\n",
        "            if num_common_skills >= min_matches:\n",
        "                common_skills_pairs.append(\n",
        "                    (current_job_number, other_job_number, list(common_skills))\n",
        "                )\n",
        "\n",
        "    skills_common_df = pd.DataFrame(\n",
        "        common_skills_pairs, columns=[\"Job Number 1\", \"Job Number 2\", \"Common Skills\"]\n",
        "    )\n",
        "    return skills_common_df"
      ],
      "metadata": {
        "id": "1N0kzJrnFFYr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import torch\n",
        "\n",
        "# Load GloVe vectors\n",
        "nlp_glove = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the GloVe vectors to GPU if available\n",
        "if device.type == \"cuda\":\n",
        "    glove_vectors = nlp_glove.vocab.vectors.data\n",
        "    glove_vectors_tensor = torch.tensor(glove_vectors, device=device)\n",
        "    nlp_glove_gpu = spacy.vocab.Vocab(vectors=spacy.vocab.Vectors(data=glove_vectors_tensor))\n",
        "    nlp_gpu = spacy.lang.en.English(vocab=nlp_glove_gpu)\n",
        "else:\n",
        "    nlp_gpu = nlp_glove\n",
        "# Update the get_embedding function to use GPU\n",
        "def get_embedding(text):\n",
        "    \"\"\"Calculate embeddings for a given text using GPU if available.\"\"\"\n",
        "    doc = nlp_glove(text)\n",
        "    if len(doc) == 0:\n",
        "        return torch.zeros(300, device=device)  # Return zeros for empty texts\n",
        "    return torch.mean(\n",
        "        torch.stack([torch.tensor(word.vector, device=device) for word in doc]), dim=0\n",
        "    )\n",
        "\n",
        "# Update the cosine_similarity function to use GPU\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Calculate cosine similarity between two vectors on GPU if available.\"\"\"\n",
        "    vec1 = vec1.to(device)\n",
        "    vec2 = vec2.to(device)\n",
        "    return torch.dot(vec1, vec2) / (vec1.norm() * vec2.norm())\n",
        "\n",
        "# Count common Skills\n",
        "def count_common_skills(common_skills_df, job_skills_df):\n",
        "    \"\"\"Count the number of jobs that share a set of common skills.\"\"\"\n",
        "    count_skills = pd.DataFrame(columns=['Count', 'Jobs', 'Common Skills'])\n",
        "\n",
        "    for index, row in common_skills_df.iterrows():\n",
        "        common_skills_set = set(row['Common Skills'])\n",
        "        count = 0\n",
        "        job_numbers = []\n",
        "\n",
        "        for job_index, job_row in job_skills_df.iterrows():\n",
        "            matched_skills_set = set(job_row['Matched Skills'])\n",
        "\n",
        "            if common_skills_set.issubset(matched_skills_set):\n",
        "                count += 1\n",
        "                job_numbers.append(job_row['Job Number'])\n",
        "\n",
        "        count_skills = pd.concat([count_skills, pd.DataFrame({\n",
        "            'Count': [count],\n",
        "            'Jobs': [job_numbers],\n",
        "            'Common Skills': [row['Common Skills']]\n",
        "        })], ignore_index=True)\n",
        "\n",
        "    return count_skills"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEK6kajmFHHB",
        "outputId": "fc705cd4-b559-4aa0-e8d9-35c11d701054"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "osn_comp_df = pd.read_csv(f\"{base_path}/osn_comp.csv\")\n",
        "osn_pub_df = pd.read_csv(f\"{base_path}/osn_public_rel.csv\")\n",
        "jobs_df = pd.read_csv(f\"{base_path}/jobs_df.csv\")\n",
        "\n",
        "# Convert OSN data to JSONL format\n",
        "convert_to_jsonl(osn_comp_df, \"osn_comp.jsonl\")\n",
        "convert_to_jsonl(osn_pub_df, \"osn_pub.jsonl\")\n",
        "\n",
        "# Fine-tuned model IDs\n",
        "fine_tuned_model_comp = \"ft:davinci-002:personal::8IIFVUbf\"\n",
        "\n",
        "# Extract and match skills\n",
        "job_skills_comp = match_skills_for_job_df(\n",
        "    jobs_df, fine_tuned_model_comp, osn_comp_df, similarity_threshold=0.65\n",
        ")\n",
        "job_skills_pub = match_skills_for_job_df(\n",
        "    jobs_df, fine_tuned_model_comp, osn_pub_df, similarity_threshold=0.65\n",
        ")\n"
      ],
      "metadata": {
        "id": "cPqqSEWgFJnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find common skills\n",
        "common_skills_comp = find_common_skills(job_skills_comp)\n",
        "common_skills_pub = find_common_skills(job_skills_pub)\n",
        "\n",
        "# Count common skills\n",
        "count_skills_comp = count_common_skills(common_skills_comp, job_skills_comp)\n",
        "count_skills_pub = count_common_skills(common_skills_pub, job_skills_pub)\n",
        "\n",
        "# Save results\n",
        "job_skills_comp.to_csv(f\"{base_path}/job_skills_comp.csv\", index=False)\n",
        "job_skills_pub.to_csv(f\"{base_path}/job_skills_pub.csv\", index=False)\n",
        "count_skills_comp.to_csv(f\"{base_path}/count_skills_comp.csv\", index=False)\n",
        "count_skills_pub.to_csv(f\"{base_path}/count_skills_pub.csv\", index=False)"
      ],
      "metadata": {
        "id": "HlD22czRUyYY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oo230DCjf-Uk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}