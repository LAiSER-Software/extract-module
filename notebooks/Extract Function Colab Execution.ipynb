{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Module Description:\n",
    "-------------------\n",
    "Class to extract skills from text and align them to existing taxonomy\n",
    "\n",
    "Ownership:\n",
    "----------\n",
    "Project: Leveraging Artificial intelligence for Skills Extraction and Research (LAiSER)\n",
    "Owner:  George Washington University Institute of Public Policy\n",
    "        Program on Skills, Credentials and Workforce Policy\n",
    "        Media and Public Affairs Building\n",
    "        805 21st Street NW\n",
    "        Washington, DC 20052\n",
    "        PSCWP@gwu.edu\n",
    "        https://gwipp.gwu.edu/program-skills-credentials-workforce-policy-pscwp\n",
    "\n",
    "License:\n",
    "--------\n",
    "Copyright 2024 George Washington University Institute of Public Policy\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n",
    "documentation files (the “Software”), to deal in the Software without restriction, including without limitation\n",
    "the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n",
    "and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n",
    "Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n",
    "WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n",
    "OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "Revision History:\n",
    "-----------------\n",
    "| Rev No. | Date | Author | Description |\n",
    "|---------|------|--------|-------------|\n",
    "| [1.0.0] | 06/05/2024 |      Satya Phanindra K. |  Created a standalone notebook for skill extraction\n",
    "| [1.0.1] | 06/11/2024 |      Satya Phanindra K. |  Added GPU support for processing\n",
    "| [1.0.1] | 06/20/2024 |      Satya Phanindra K. |  Added error handling and logging\n",
    "| [1.0.2] | 07/01/2024 |      Satya Phanindra K. |  Threshold update for similarity and AI model\n",
    "| [1.0.3] | 07/10/2024 |      Satya Phanindra K. |  Added seperate functions set for LLM usecases\n",
    "| [1.0.4] | 07/13/2024 |      Satya Phanindra K. |  Add descriptions to each method\n",
    "| [1.0.5] | 07/18/2024 |      Satya Phanindra K. |  Added CONDITIONAL GPU support for LLM\n",
    "| [1.0.6] | 07/22/2024 |      Satya Phanindra K. |  Added support for SkillNer model for skill extraction, if GPU not available\n",
    "| [1.0.7] | 07/25/2024 |      Satya Phanindra K. |  Calculate cosine similarities in bulk for optimal performance.\n",
    "| [1.0.8] | 07/28/2024 |      Satya Phanindra K. |  Error handling for empty list outputs from extract_raw function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXSNwb_tWbFV"
   },
   "source": [
    "## Installing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/LAiSER-Software/extract-module/refs/heads/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dAXhCo53ZZ6u",
    "outputId": "17dd5889-3ac3-4567-9dc8-e16eb7eb5af3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6d3giOVWh4m"
   },
   "source": [
    "## Import essential packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZFDH_aukkFeG",
    "outputId": "eb4c68d4-c14c-4582-ad31-e6abe9c83d64"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dJh8ut-XkmkT"
   },
   "outputs": [],
   "source": [
    "# native packages\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# installed packages\n",
    "import time\n",
    "import spacy\n",
    "import torch\n",
    "import pandas as pd\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from skillNer.general_params import SKILL_DB\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from skillNer.skill_extractor_class import SkillExtractor\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msg-_umMkmkW"
   },
   "source": [
    "## Global variables and Function Definitions from internal packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFagPCVfkmkZ"
   },
   "source": [
    "#### Utils\n",
    "\n",
    "- The content in the code cell below are copied from the `../laiser/utils.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qhGrJNJ2kmkc"
   },
   "outputs": [],
   "source": [
    "#utils\n",
    "import numpy as np\n",
    "import psutil\n",
    "import logging\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates cosine similarity between 2 vectors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1, vec2 : numpy array of vectorized text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numeric value\n",
    "    \"\"\"\n",
    "    product_of_magnitude = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    if product_of_magnitude == 0.0:\n",
    "        return 0.0\n",
    "    return np.dot(vec1, vec2) / product_of_magnitude\n",
    "\n",
    "\n",
    "def get_embedding(nlp, input_text):\n",
    "    \"\"\"\n",
    "    Creates vector embeddings for input text based on nlp object\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nlp : object of spacy nlp model\n",
    "    input_text : text\n",
    "        Provide text to be vectorized, usually skill, extracted of referenced\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy array of vectorized text\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    doc = nlp(input_text)\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(300)  # Return zeros for empty texts\n",
    "    return np.mean([word.vector for word in doc], axis=0)\n",
    "\n",
    "\n",
    "def log_performance(function_name, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Utility function to log performance in unit of time for a function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    function_name : text\n",
    "        Name of the function\n",
    "    start_time : time\n",
    "        execution start time of the function\n",
    "    end_time : time\n",
    "        execution end time of the function\n",
    "\n",
    "    \"\"\"\n",
    "    execution_time = end_time - start_time\n",
    "    process = psutil.Process()\n",
    "    cpu_percent = process.cpu_percent()\n",
    "    memory_info = process.memory_info()\n",
    "    memory_usage = memory_info.rss / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    log_message = (\n",
    "        f\"Function: {function_name}\\n\"\n",
    "        f\"Execution time: {execution_time:.2f} seconds\\n\"\n",
    "        f\"CPU usage: {cpu_percent:.2f}%\\n\"\n",
    "        f\"Memory usage: {memory_usage:.2f} MB\\n\"\n",
    "        \"-------------------------------\"\n",
    "    )\n",
    "    logging.info(log_message)\n",
    "    print(log_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEfwgGXqkmkh"
   },
   "source": [
    "#### Params\n",
    "- The content in the code cell below are copied from the `../laiser/params.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCf46SzXkmki"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use a placeholder if __file__ is not available\n",
    "if '__file__' in locals():\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "else:\n",
    "    ROOT_DIR = os.getcwd()  # Use current working directory as a fallback\n",
    "\n",
    "INPUT_PATH = os.path.join(ROOT_DIR, 'input')\n",
    "OUTPUT_PATH = os.path.join(ROOT_DIR, 'output')\n",
    "\n",
    "# SKILL_DB_PATH = os.path.join(INPUT_PATH, 'combined.csv')\n",
    "SKILL_DB_PATH = 'https://raw.githubusercontent.com/phanindra-max/LAiSER-datasets/master/combined.csv'\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.85\n",
    "\n",
    "AI_MODEL_ID = 'google/gemma-2b-it'\n",
    "\n",
    "from google.colab import userdata\n",
    "HF_TOKEN = userdata.get(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHNl6ruHkmkl"
   },
   "source": [
    "#### LLM Methods\n",
    "- The content in the code cell below are copied from the `../laiser/llm_methods.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BA9j14W9kmkm"
   },
   "outputs": [],
   "source": [
    "# llm_methods\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def fetch_model_output(response):\n",
    "    # Find the content between the model start tag and the last <eos> tag\n",
    "    pattern = r'<start_of_turn>model\\s*<eos>(.*?)<eos>\\s*$'\n",
    "    match = re.search(pattern, response, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        content = match.group(1).strip()\n",
    "\n",
    "        # Split the content by lines and filter out empty lines\n",
    "        lines = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "\n",
    "        # Extract skills (lines starting with '-')\n",
    "        skills = [line[1:].strip() for line in lines if line.startswith('-')]\n",
    "\n",
    "        return skills\n",
    "\n",
    "def get_completion_batch(queries: list, model, tokenizer, batch_size=2) -> list:\n",
    "    device = \"cuda:0\"\n",
    "    results = []\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    <start_of_turn>user\n",
    "    Name all the skills present in the following description in a single list. Response should be in English and have only the skills, no other information or words. Skills should be keywords, each being no more than 3 words.\n",
    "    Below text is the Description:\n",
    "\n",
    "    {query}\n",
    "    <end_of_turn>\\n<start_of_turn>model\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i+batch_size]\n",
    "        prompts = [prompt_template.format(query=query) for query in batch]\n",
    "\n",
    "        encodeds = tokenizer(prompts, return_tensors=\"pt\", add_special_tokens=True, padding=True, truncation=True)\n",
    "        model_inputs = encodeds.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n",
    "\n",
    "        for full_output in decoded:\n",
    "            # Extract only the model's response\n",
    "            response = full_output.split(\"<start_of_turn>model<eos>\")[-1].strip()\n",
    "            processed_response = fetch_model_output(response)\n",
    "            results.append(processed_response)\n",
    "\n",
    "        # Clear CUDA cache after each batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Processed batch {i//batch_size + 1}/{(len(queries)-1)//batch_size + 1}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    <start_of_turn>user\n",
    "    Name all the skills present in the following description in a single list. Response should be in English and have only the skills, no other information or words. Skills should be keywords, each being no more than 3 words.\n",
    "    Below text is the Description:\n",
    "\n",
    "    {query}\n",
    "    <end_of_turn>\\n<start_of_turn>model\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.format(query=query)\n",
    "\n",
    "    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    model_inputs = encodeds.to(device)\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "    response = decoded.strip()\n",
    "    processed_response = fetch_model_output(response)\n",
    "    return (processed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNnWf4lmkmkp"
   },
   "source": [
    "## Skill Extractor Class\n",
    "\n",
    "- Copied from `../laiser/skill_extractor.py` file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_r3Q5-Kkkmkq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class Skill_Extractor:\n",
    "    \"\"\"\n",
    "    Class to extract skills from text and align them to existing taxonomy\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    client : HuggingFace API client\n",
    "    nlp : spacy nlp model\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    extract_raw(input_text: text)\n",
    "        The function extracts skills from text using NER model\n",
    "\n",
    "    align_skills(raw_skills: list, document_id='0': string):\n",
    "        This function aligns the skills provided to the desired taxonomy\n",
    "\n",
    "    extractor(data: pandas dataframe, id_column='Research ID', text_column='Text'):\n",
    "        Function takes text dataset to extract and aligns skills based on available taxonomies\n",
    "    ....\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_id = AI_MODEL_ID\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.skill_db_df = pd.read_csv(SKILL_DB_PATH)\n",
    "        self.skill_db_embeddings = np.array([get_embedding(self.nlp, label) for label in self.skill_db_df['SkillLabel']])\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"GPU is available. Using GPU for Fine-tuned Language model initialization.\")\n",
    "            torch.cuda.empty_cache()\n",
    "            self.bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            )\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_id,\n",
    "                quantization_config=self.bnb_config,\n",
    "                device_map={\"\": 0},\n",
    "                token=HF_TOKEN,\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, add_eos_token=True, padding_side='left', token=HF_TOKEN)\n",
    "        else:\n",
    "            print(\"GPU is not available. Using CPU for SkillNer model initialization.\")\n",
    "            self.ner_extractor = SkillExtractor(self.nlp, SKILL_DB, PhraseMatcher)\n",
    "        return\n",
    "\n",
    "    # Declaring a private method for extracting raw skills from input text\n",
    "    def extract_raw(self, input_text):\n",
    "        \"\"\"\n",
    "        The function extracts skills from text using Fine-Tuned Language Model's API\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_text : text\n",
    "            Job advertisement / Job Description / Syllabus Description / Course Outcomes etc.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list: List of extracted skills from text\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        More details on which (pre-trained) language model is fine-tuned can be found in llm_methods.py\n",
    "        The Function is designed only to return list of skills based on prompt passed to OpenAI's Fine-tuned model.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            # GPU is available. Using Language model for extraction.\n",
    "            extracted_skills = get_completion(input_text, self.model, self.tokenizer)\n",
    "            print(\"Extracted_skills: \", extracted_skills)\n",
    "            extracted_skills_set = set(extracted_skills)\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # GPU is not available. Using SkillNer model for extraction.\n",
    "            ner_extractor = self.ner_extractor\n",
    "            extracted_skills_set = set()\n",
    "            annotations = None\n",
    "            try:\n",
    "                annotations = ner_extractor.annotate(input_text)\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping example, ValueError encountered: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping example, An unexpected error occurred: {e}\")\n",
    "\n",
    "            for item in annotations['results']['full_matches']:\n",
    "                extracted_skills_set.add(item['doc_node_value'])\n",
    "\n",
    "            # get ngram_scored\n",
    "            for item in annotations['results']['ngram_scored']:\n",
    "                extracted_skills_set.add(item['doc_node_value'])\n",
    "\n",
    "        return list(extracted_skills_set)\n",
    "\n",
    "    def align_skills(self, raw_skills, document_id='0'):\n",
    "        \"\"\"\n",
    "        This function aligns the skills provided to the available taxonomy\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_skills : list\n",
    "            Provide list of skill extracted from Job Descriptions / Syllabus.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list: List of taxonomy skills from text in JSON format\n",
    "            [\n",
    "                {\n",
    "                    \"Research ID\": text_id,\n",
    "                    \"Skill Name\": Raw skill extracted,\n",
    "                    \"Skill Tag\": taxonomy skill tag,\n",
    "                    \"Correlation Coefficient\": similarity_score\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "\n",
    "        \"\"\"\n",
    "        raw_skill_embeddings = np.array([get_embedding(self.nlp, skill) for skill in raw_skills])\n",
    "\n",
    "        # Calculate cosine similarities in bulk\n",
    "        similarities = 1 - cdist(raw_skill_embeddings, self.skill_db_embeddings, metric='cosine')\n",
    "\n",
    "        matches = []\n",
    "        for i, raw_skill in enumerate(raw_skills):\n",
    "            skill_matches = np.where(similarities[i] > SIMILARITY_THRESHOLD)[0]\n",
    "            for match in skill_matches:\n",
    "                matches.append({\n",
    "                    \"Research ID\": document_id,\n",
    "                    \"Raw Skill\": raw_skill,\n",
    "                    \"Skill Tag\": self.skill_db_df.iloc[match]['SkillTag'],\n",
    "                    \"Correlation Coefficient\": similarities[i, match]\n",
    "                })\n",
    "\n",
    "        return matches\n",
    "\n",
    "    def extractor(self, data, id_column='Research ID', text_column='Text'):\n",
    "        \"\"\"\n",
    "        Function takes text dataset to extract and aligns skills based on available taxonomies\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas dataframe\n",
    "            Dataset containing text id and actual text to extract skills.\n",
    "        id_column: string\n",
    "            Name of id column in the dataset. Defaults to 'Research ID'\n",
    "        text_column: string\n",
    "            Name of the text column in the dataset. Defaults to 'Text'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list: List of skill tags and similarity_score for all texts in  from text in JSON format\n",
    "            [\n",
    "                {\n",
    "                    \"Research ID\": text_id\n",
    "                    \"Skill Name\": Raw skill extracted,\n",
    "                    \"Skill Tag\": taxonomy skill tag,\n",
    "                    \"Correlation Coefficient\": similarity_score\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        extracted = pd.DataFrame(columns=['Research ID', 'Raw Skill', 'Skill Tag', 'Correlation Coefficient'])\n",
    "        for index, row in data.iterrows():\n",
    "            research_id = row[id_column]\n",
    "            input_text = row[text_column]\n",
    "            raw_skills = self.extract_raw(input_text)\n",
    "            if(len(raw_skills) == 0):\n",
    "                continue\n",
    "            else:\n",
    "              aligned_skills = self.align_skills(raw_skills, research_id)\n",
    "              extracted = extracted._append(aligned_skills, ignore_index=True)\n",
    "        end_time = time.time()\n",
    "        log_performance('extractor', start_time, end_time)\n",
    "        return extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oi0xtdQ3kmks"
   },
   "source": [
    "## Using the Skill Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "E1tEcB68kmks"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering 1 rows for processing...\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset\n",
    "nlx_sample = pd.read_csv('https://raw.githubusercontent.com/phanindra-max/LAiSER-datasets/master/nlx_tx_sample_data_gwu.csv')\n",
    "\n",
    "nlx_sample = nlx_sample[0:1]\n",
    "nlx_sample = nlx_sample[['description', 'job_id']]\n",
    "print(\"Considering\", len(nlx_sample), \"rows for processing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "_Hj780rskmku",
    "outputId": "7b98ab31-dae7-45b7-dc26-323366703fd9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>job_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Req ID: 29534BR\\n\\nPOSITION SUMMARY\\n\\nThis po...</td>\n",
       "      <td>69322097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description    job_id\n",
       "0  Req ID: 29534BR\\n\\nPOSITION SUMMARY\\n\\nThis po...  69322097"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlx_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150,
     "referenced_widgets": [
      "b77c98f1aec1464a893654a796e43fc4",
      "a59d8649480e43f79da975ef64cdfee3",
      "7519c70fcfb040deaadc2ee57e38b02e",
      "25468a9dc2fa4a8fbf0fe46d6eb02ab4",
      "2874102999524fa68c5ac3030cd58dd8",
      "0d39e1e2930a470589f0732b7cf802cc",
      "77bee6fcdf444ceb9306eec56356e67a",
      "e595239e0fa045138b73428b801e9fe9",
      "ed7dd01f758846feb53886cc93299a2f",
      "9a5c2ff141034180b94827fcfb1f9030",
      "b5669d63c0dd46fbb7812cde3b365881"
     ]
    },
    "id": "wc1O-yXBkmkt",
    "outputId": "7c532ac8-3150-43cf-dba2-d4b415fefd91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available. Using CPU for SkillNer model initialization.\n",
      "loading full_matcher ...\n",
      "loading abv_matcher ...\n",
      "loading full_uni_matcher ...\n",
      "loading low_form_matcher ...\n",
      "loading token_matcher ...\n"
     ]
    }
   ],
   "source": [
    "# creating an object from the Extract class\n",
    "se = Skill_Extractor() # runs __init__() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "K_FbXZ4Ckmkv",
    "outputId": "42cf78d6-ae64-4f0c-adde-6a810fdd5ed0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skillNer\\utils.py:99: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  vec_similarity = token1.similarity(token2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: extractor\n",
      "Execution time: 22.05 seconds\n",
      "CPU usage: 0.00%\n",
      "Memory usage: 2999.23 MB\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phani\\AppData\\Local\\Temp\\ipykernel_5056\\33541570.py:180: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  extracted = extracted._append(aligned_skills, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# skills output based on the taxonomy database\n",
    "output = se.extractor(nlx_sample, 'job_id', 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "i-lisuxSkmkx",
    "outputId": "6cc5587f-ded0-4e77-f617-ae98aa372670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Research ID               Raw Skill  Skill Tag  Correlation Coefficient\n",
      "0     69322097              operations   ESCO.805                 0.850277\n",
      "1     69322097  business communicating   ESCO.113                 0.916486\n",
      "2     69322097  business communicating   ESCO.761                 0.872133\n",
      "3     69322097  business communicating   ESCO.835                 0.886956\n",
      "4     69322097                 medical   ESCO.892                 0.924566\n",
      "..         ...                     ...        ...                      ...\n",
      "86    69322097     maintenance testing  ESCO.1278                 0.884750\n",
      "87    69322097                     CSS  ESCO.1150                 1.000000\n",
      "88    69322097                business   ESCO.113                 0.879549\n",
      "89    69322097                business   ESCO.761                 0.889712\n",
      "90    69322097                business   ESCO.835                 0.873538\n",
      "\n",
      "[91 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# save the extracted skills to a csv file\n",
    "print(output)\n",
    "output.to_csv('extracted_skills_for_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4AUKwdJNK_7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d39e1e2930a470589f0732b7cf802cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25468a9dc2fa4a8fbf0fe46d6eb02ab4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a5c2ff141034180b94827fcfb1f9030",
      "placeholder": "​",
      "style": "IPY_MODEL_b5669d63c0dd46fbb7812cde3b365881",
      "value": " 2/2 [00:26&lt;00:00, 11.07s/it]"
     }
    },
    "2874102999524fa68c5ac3030cd58dd8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7519c70fcfb040deaadc2ee57e38b02e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e595239e0fa045138b73428b801e9fe9",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed7dd01f758846feb53886cc93299a2f",
      "value": 2
     }
    },
    "77bee6fcdf444ceb9306eec56356e67a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a5c2ff141034180b94827fcfb1f9030": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a59d8649480e43f79da975ef64cdfee3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d39e1e2930a470589f0732b7cf802cc",
      "placeholder": "​",
      "style": "IPY_MODEL_77bee6fcdf444ceb9306eec56356e67a",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "b5669d63c0dd46fbb7812cde3b365881": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b77c98f1aec1464a893654a796e43fc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a59d8649480e43f79da975ef64cdfee3",
       "IPY_MODEL_7519c70fcfb040deaadc2ee57e38b02e",
       "IPY_MODEL_25468a9dc2fa4a8fbf0fe46d6eb02ab4"
      ],
      "layout": "IPY_MODEL_2874102999524fa68c5ac3030cd58dd8"
     }
    },
    "e595239e0fa045138b73428b801e9fe9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed7dd01f758846feb53886cc93299a2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
