{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f87pSKZOtJSS"
      },
      "source": [
        "## Finetuning the Gemma LLM by Google for the LAiSER Research Work\n",
        "\"\"\"\n",
        "Notebook Description:\n",
        "-------------------\n",
        "Fine-tuning a Language Model for extracting skill keywords\n",
        "\n",
        "Ownership:\n",
        "----------\n",
        "Project: Leveraging Artificial intelligence for Skills Extraction and Research (LAiSER)\n",
        "Owner:  George Washington University Institute of Public Policy\n",
        "        Program on Skills, Credentials and Workforce Policy\n",
        "        Media and Public Affairs Building\n",
        "        805 21st Street NW\n",
        "        Washington, DC 20052\n",
        "        PSCWP@gwu.edu\n",
        "        https://gwipp.gwu.edu/program-skills-credentials-workforce-policy-pscwp\n",
        "\n",
        "License:\n",
        "--------\n",
        "Copyright 2024 George Washington University Institute of Public Policy\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n",
        "documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation\n",
        "the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n",
        "and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n",
        "Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n",
        "WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
        "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n",
        "OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\n",
        "\n",
        "Input Requirements:\n",
        "-------------------\n",
        "- Taxonomy dataset\n",
        "- Job/Course description data\n",
        "\n",
        "Output/Return Format:\n",
        "----------------------------\n",
        "- List of Skill Keywords\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Revision History:\n",
        "-----------------\n",
        "Rev No.     Date            Author              Description\n",
        "[1.0.0]     06/18/2024      Satya Phanindra K.  Setup and run Gemma-2b-it\n",
        "[1.0.3]     06/20/2024      Satya Phanindra K.  Fine-tune the model and push to HuggingFace\n",
        "[1.0.4]     06/21/2024      Satya Phanindra K.  Import and use the fine-tuned model\n",
        "\n",
        "TODO:\n",
        "-----\n",
        "- 1: huggingface import should use GPU for execution\n",
        "- 2: Run the model against 10-15 job descriptions\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwS-1-U_ibSu"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before delving into the fine-tuning process, ensure that you have the following prerequisites in place:\n",
        "\n",
        "1. **GPU**: [gemma-2b](https://huggingface.co/google/gemma-2b) - can be finetuned on T4(free google colab) while [gemma-7b](https://huggingface.co/google/gemma-7b) requires an A100 GPU.\n",
        "2. **Python Packages**: Ensure that you have the necessary Python packages installed. You can use the following commands to install them:\n",
        "\n",
        "Let's begin by checking if your GPU is correctly detected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ37j57QibSv",
        "outputId": "5931289d-6f2c-42ca-9516-0eff44897aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jun 21 14:51:21 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uss51g5ibSx"
      },
      "source": [
        "## Step 2 - Model loading\n",
        "We'll load the model using QLoRA quantization to reduce the usage of memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wXaD_P6ibSy",
        "outputId": "731238ce-0317-4e50-b90b-c75359c69998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 2337, in parseImpl\n",
            "    if instring[loc] == self.firstMatchChar and instring.startswith(\n",
            "IndexError: string index out of range\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 229, in _get_updated_criteria\n",
            "    for requirement in self._p.get_dependencies(candidate=candidate):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/provider.py\", line 244, in get_dependencies\n",
            "    return [r for r in candidate.iter_dependencies(with_requires) if r is not None]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/provider.py\", line 244, in <listcomp>\n",
            "    return [r for r in candidate.iter_dependencies(with_requires) if r is not None]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 491, in iter_dependencies\n",
            "    valid_extras = self.extras.intersection(self.base.dist.iter_provided_extras())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 219, in iter_provided_extras\n",
            "    return self._dist.extras\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3064, in extras\n",
            "    return [dep for dep in self._dep_map if dep]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 856, in _parseNoCache\n",
            "    tokens = fn(instring, tokens_start, ret_tokens)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 291, in wrapper\n",
            "    ret = func(*args[limit:])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 71, in <lambda>\n",
            "    lambda s, l, t: Marker(s[t._original_start : t._original_end])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 278, in __init__\n",
            "    self._markers = _coerce_parse_result(MARKER.parseString(marker))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 5226, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4891, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4790, in parseImpl\n",
            "    loc, tokens = self_expr_parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 819, in _parseNoCache\n",
            "    raise ParseException(instring, len_instring, self.errmsg, self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/exceptions.py\", line 24, in __init__\n",
            "    def __init__(\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1514, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -q -U bitsandbytes==0.42.0\n",
        "!pip3 install -q -U peft==0.8.2\n",
        "!pip3 install -q -U trl==0.7.10\n",
        "!pip3 install -q -U accelerate==0.27.1\n",
        "!pip3 install -q -U datasets==2.17.0\n",
        "!pip3 install -q -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIoLQMomibSz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw_HKUa2ibS1"
      },
      "source": [
        "Now we specify the model ID and then we load it with our previously defined quantization configuration.Now we specify the model ID and then we load it with our previously defined quantization configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h914x-ZQibS2"
      },
      "outputs": [],
      "source": [
        "# if you are using google colab\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "ddefebf9faae4272b968223eacf0bb0e",
            "c361efeb14c84d8ca08bf803c35a7e3e",
            "a09cfa43d95b4611a2f351b5c95b68ea",
            "6813cb5a38a9435396ca892fe634ccf4",
            "09ed01c35363471c830be9c527115526",
            "5f9af024e058455197da9446e7953443",
            "06b0092f4afb4657a7b7eaf7c504f5cf",
            "30738849c6474e57bbc820a38fab4efb",
            "098f95542c1e4fa99bbb9c06bcd1e062",
            "70250a94e32548dd88a3a58cf3dc1933",
            "a4f6fe1f276b4b6994ce1abaf4bb8be6",
            "10d6cee5ffcf4c29b11c778e2401aa50",
            "c17e104b0d6644eeb8dcfa38af67013b",
            "1e0db0f2296b419f97656afae4a9701f",
            "a869a38ed2da41c0906094c9594b0e15",
            "4e20bd5f19174f609cbce23f7cc76c7d",
            "62bce7d7a6474d29936fa3d74b429204",
            "ca47a0d01a92431688b9b00ac715c74a",
            "9631dc58985d45a1a028506c5eb71d5f",
            "b447ed94469a409bbc361a0c796226ac",
            "1f112a6174194bb4b1f011ef03fbbf9a",
            "e02af7631df943938891f4638d94c60b",
            "094d49c630524c05925e0acac48d9772",
            "a851b182439b4caaacba5bded86c9ccc",
            "cd4bd83da7cb4e5e9a2245673e883ae5",
            "8a7f19202c8d4cabbd2263f2f3df4751",
            "88b86973a176495c95f6ef64f24a9bd4",
            "8e7707e7e7234063972f84b2ee42de9d",
            "47dee294ee8b4d9ab24d8485ee7c9d4e",
            "2f5485443546406395584cf7feae4b19",
            "133b514d4dc54742aa1ef686f707526c",
            "8f01c3a84ff941508619b41e854c101c",
            "f544f02abd4240e7a77e1438f96d1354",
            "ef321f72e0d04ba98cf69bf0e538444e",
            "354cc8cc77be47919d931d277579ed85",
            "d0bd0984ff9f4b8287cc8a30c0eac454",
            "9c8ca939c6e14d789ebfba5ce9499536",
            "abfcfcc2e6ca448e9c0e336ae6f6bcf2",
            "1ddfb987e5774be9a802570b17ea33a7",
            "d5d2c51d573741de86223e074391ac9e",
            "49ac89b4b17747d4ab630973b2dbce68",
            "ea824af9f11142f3a1373e5f16f101de",
            "761ad5df29504979aeba0b1fdff62b0b",
            "a5e646c208504e218aaa8d9025fac410",
            "34b9bddd74f54fb3bd2c6b16e5a82e0b",
            "84a4758511a742c193cf98a5e886b10a",
            "a645897a9cb54f8e9535e6c76014b89d"
          ]
        },
        "id": "Mii7j4_pibS7",
        "outputId": "0987fd60-23e8-4caa-e058-d853bf7ca8ef"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddefebf9faae4272b968223eacf0bb0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx3ygini9R-2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
          "referenced_widgets": [
            "877b7e268a2a4aac801fefa381bb3690",
            "2af5ea901def4502ba12c44163484399",
            "0ddff5f8d67b452795d7ccf543a0d1c5",
            "72f95f69a2234113851fa137cd5181dd",
            "d930e44e8dd44c119d76d2274ae61d09",
            "1b780933f4324f05a55884a9eefb5d3b",
            "dbb02e87ef534c28a09bf83bdad1aa34",
            "0fe2f8a6deae4f27b5c488b411a149db",
            "20063e2e15dd4866ac07e1af9b8b77e8",
            "cd9b2c6ca7ac4ca98d84a870d83170ba",
            "53d34db029004a8a90e557fb6f86e355",
            "442b4575e8c441eea6244c28dca5bd50",
            "f7ce624f5ed54bc7bc8e79e6741a27cd",
            "c6775fe6cff340b09959131cbde185bd",
            "254b3c0beeca4b869a6e80a4dba14522",
            "a328fbed48044ece9440beb7843b3152",
            "43e61eb2195b48658de7938d7ea5eddc",
            "10feb6c5c3844ef3ba22ceafa2f8c832",
            "25672dd11aa546b7a0f62cf84c7f550f",
            "ad3eb856e6ad4bb19c42461d2bfe9d51",
            "1a4268b2fd1e49d6ac8c5127b83d079b",
            "259ecbd4d7164319babfcd2fff86c7fe",
            "28233fe203bd47929c144b1b0dd20ac9",
            "40943b97e4db4608a808b5287734c1a3",
            "5e70979d34df430bb66a972e4b5f32f2",
            "4a514ae3df72469fa41736ff498e4e4c",
            "80bc6ae3f37b48a69ef32725ee0650b7",
            "d1fa9d85f9ff4067a3ab6b9c79c60926",
            "9a38a89cd68e4c4cb1e8ddfa571b8a54",
            "73b2dadc63ac4e729b96ce18e97ebe4f",
            "dfeeaedaff494a91bfc5e226dc4767ac",
            "7f7f69635033453ea62e039e1aaec188",
            "7f0cbf1dd3cd4d7d9c351603c5b63707",
            "5c7901a82de046288cdff1c5e91d9287",
            "fd5956b3ab254a00bf2ed6a06da83f9f",
            "cb073c6224c94be2b9de6f35e450c6fa",
            "fde6add5c86749178370805975cc06a9",
            "d4f34742193445df9d3282657334dded",
            "c1cf9e2a84d345ef83ddfae7c28304ee",
            "d85a63b164c14d53a05a6c07efd89e63",
            "f550210724f94070816066b3e6d55b4b",
            "8ae1b93ea8cb4e32aac1995860f81ce2",
            "e4a6531128574ee9b2579305c0dcc89c",
            "549279ece036477081dcfaf8b824a3a3",
            "e2451a531ea14e898f1b33dbeba6df96",
            "f1cd3e92102645ed87acbc3e8dc50a9d",
            "4aec0464ca584f34bc4f43f2b0470d56",
            "39738f6236d149eebe49781639e60ee3",
            "9c6833eb61314038968f759fd6c46e02",
            "c6d845c1517f428abb754e8f2e181740",
            "962f76ba361f4453a8eca7ea96fae0c8",
            "3c7451d03a38449dbe33b09c97e4e34e",
            "4bce36a934c940ab932a6a7e9f60baa0",
            "00c8a0df040b45e6a2455a0d2e6def6d",
            "fa8f7e4c518b4099836a1848c9f1f967",
            "42f58d9cd9114160934964fd091f7193",
            "0ac879be6f464d3baae266d2e1ffd16e",
            "2915d4d26e594d1e978589e8ad6c3dfb",
            "863c4d16d971472598ba07b97a6672b4",
            "ade91d9eb8234715a7b28660511f867f",
            "a7b0c0aca23740a79e3958bff4d50bda",
            "c0ec88d0456f4d53bd3b9e5215703f4c",
            "4f27018461a446a1a29fa3d1389d70a0",
            "7f27373bf87f420a8bea709847087c1d",
            "f87d58505d794fe184d847ab1ab36970",
            "26cd1641b2b643e2b0189e163e421bec",
            "129181446a154086b707b05e0f71b24d",
            "93a6c22419e146d4a5212145152f9735",
            "2bbd2f4a9c8f41c3b7e8e0aa8f727f81",
            "151afc7839b34a9bb48616e89d853110",
            "74921ef8a90f41bfbcca0160da7d2317",
            "ef10985b17d14a5d9f32b0e4306f4183",
            "0d1701f7066a4db689425a0ba1907c69",
            "1b7f2fdb1eef4365930ee8299826bfea",
            "da3ed00c20f6416db574760e5c475ee9",
            "ee9bf12de8704c06a3ee58e909cb1cda",
            "64eb39a5e9d84013b67b444ccc01cc8c",
            "3eaaa316faa94f399328fe66403d7782",
            "3fc482710c194b89babc2de6c95f49a5",
            "70fc5c52597f4326a48734e50b0c1e2d",
            "37342373fb6245a990547a9ca4e09c9e",
            "b6f0e078e5b9495fb724fc376a089c20",
            "cfdcde1b9341405492b70dcb58e658ad",
            "00b56c738fa54bd193a9120810692f3b",
            "c038c1e3698e4eed97aff3f6b69815de",
            "309b5954f762474ea2ad4eaacb62f35b",
            "9028465f8e224282b899aee840f3cd16",
            "a240666a0d704249ab4984b4e9194971",
            "e7bff5db0c194b79b7b27894c8f5959e",
            "eb78349b94f745eb8b8104c506ea6db1",
            "a3ffdee5927041168be885729d6ee595",
            "6279a5c7d5744c1aaa42eea269397dab",
            "9810d851e40a4f8593f27b4c827fbbcd",
            "6090d25d2ca34387ba5563c1cacf32c9",
            "3adc3cb8bf9a42f2a97df70f7862752f",
            "e912bdd8042b47a29f242e6f2494268e",
            "6ab28dbee9f547fb9a105c9a45c336c7",
            "7e68a12f87e74468a224677545ced43a",
            "ee35305258414596914a6e556aa767f3",
            "9b2ea5eb32e142bfa152cb9c8cacc3a3",
            "b659e67dfce4482ea14d046b13fcd58f",
            "ec247845da61443ebda97927288e0332",
            "0ac7a766e0b94bd4b4c5ba973b49a633",
            "e0a1077a50da44dcbed3346fc339f2a1",
            "d392fa4f1d3a4d31bc882afd6fa54ea0",
            "33ba23bf0beb44c7bb39464230607d95",
            "37ec117932904931b75db0098b796b56",
            "7a9295ac51cb43ecb52bb0f8770fd16f",
            "f6b1b40958054968ab74e528ddef7715",
            "af985b10d7804621878fe36885c10bb6",
            "0383aa96b8e543759d6f83ae8ae12885",
            "6947c048b6fe467e89b6bd3ec9a54fd3",
            "2482774026c74f8c87709d780281c2a9",
            "3ec574c38ca5463d8b69148848735fde",
            "df2a6bb4883a4e168d2aedd3d3a33c40",
            "ee13b9184f88495e8b3b907e96af8062",
            "11458bfaff734d1aaae6e0e05fcb8b47",
            "262cc0a4a5ea4178bf3d87a4bc033191",
            "ef6abb2bbd5b4e969fdf83e5fa7cc8d2",
            "faacaea064ea4af489bf168e16e747c9",
            "5b7b3d982f16454c9dc3652be1bbead7"
          ]
        },
        "id": "Hsl67bq-ibS-",
        "outputId": "e6047ed5-39f5-44b1-82a2-06c2ba397e4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "877b7e268a2a4aac801fefa381bb3690",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "442b4575e8c441eea6244c28dca5bd50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28233fe203bd47929c144b1b0dd20ac9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c7901a82de046288cdff1c5e91d9287",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2451a531ea14e898f1b33dbeba6df96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42f58d9cd9114160934964fd091f7193",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "129181446a154086b707b05e0f71b24d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eaaa316faa94f399328fe66403d7782",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7bff5db0c194b79b7b27894c8f5959e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b2ea5eb32e142bfa152cb9c8cacc3a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0383aa96b8e543759d6f83ae8ae12885",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# model_id = \"google/gemma-7b-it\"\n",
        "# model_id = \"google/gemma-7b\"\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "# model_id = \"google/gemma-2b\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWo0RR6GibS_"
      },
      "outputs": [],
      "source": [
        "def get_completion(query: str, model, tokenizer) -> str:\n",
        "  device = \"cuda:0\"\n",
        "\n",
        "  prompt_template = \"\"\"\n",
        "  <start_of_turn>user\n",
        "  Name all the skills present in the following description in a single list. Response should be in English and have only the skills, no other information or words. Skills should be keywords, each being no more than 3 words.\n",
        "  Below text is the Description:\n",
        "\n",
        "  {query}\n",
        "  <end_of_turn>\\n<start_of_turn>model\n",
        "\"\"\"\n",
        "  prompt = prompt_template.format(query=query)\n",
        "\n",
        "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "\n",
        "\n",
        "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "  # decoded = tokenizer.batch_decode(generated_ids)\n",
        "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "  return (decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6to3tT4ibTA",
        "outputId": "72106584-53ca-4f86-8342-f4753f7aefe9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  user\n",
            "  Name all the skills present in the following description in a single list. Response should have only the skills, no other information or words. Skills should be keywords, each being no more than 3 words.\n",
            "  Below text is the Description:\n",
            "\n",
            "  SANCORP is seeking FTE Level II Data Scientist to support the office of DoD Chief Digital and Artificial Intelligence Office (CDAO) Chief Technology Officer (CTO). CDAO CTO requires support in multiple functional areas to ensure deliverables associated with the CDAO Architecture Council, CTO Federation, and CTO Future Architecture Activities. The mission of the CDAO CTO is to accelerate the DoD's adoption of data, analytics, and AI to improve decision making across all levels of the department. The following are examples of responsibilities:\n",
            "          Support development of insider threat strategy in support of protecting CDAO technical offerings; balance short-term wins with long-term investments to progressively mature CDAO\u2019s defenses against insider threats.\n",
            "          Lead coordination of policy and strategy related to insider threats with industry partners and other DoD components.\n",
            "          Lead exploration of data sources that are relevant to measuring, identifying, and defending against insider threats.\n",
            "          Provide technical leadership in developing capabilities to detect insider threats among large user communities, leveraging combination of statistical, classical machine learning, and deep learning methods.\n",
            "          Sancorp Consulting LLC shall, in its discretion, modify or adjust the position to meet Sancorp\u2019s changing needs. This job description is not a contract and may be adjusted as deemed appropriate at Sancorp\u2019s sole discretion.\n",
            "          Sancorp Consulting, LLC, is an SDVOSB and SBA 8(a) company seeking highly motivated and qualified professionals and offer an attractive salary and benefits package that includes: Medical, Dental, life and Disability Insurance; 401K, and holidays to ensure the highest quality of life for our employees. Please visit our website for more information at www.sancorpconsulting.com.\n",
            "          Sancorp Consulting, LLC is an equal opportunity employer. At Sancorp Consulting, LLC we are committed to providing equal employment opportunities (EEO) to all employees and applicants without regard to race color, religion, sex, national origin, age, disability, or any other protected characteristic as defined by applicable law. We strive to create an inclusive and diverse workplace where everyone feels valued, respected, and supported.\"\"\"\n",
            "          \n",
            "  \n",
            "model\n",
            "Sure, here is the list of skills:\n",
            "\n",
            "- insider threat strategy\n",
            "- policy and strategy\n",
            "- data source exploration\n",
            "- statistical analysis\n",
            "- machine learning\n",
            "- deep learning\n",
            "- incident detection\n"
          ]
        }
      ],
      "source": [
        "query_text = '''SANCORP is seeking FTE Level II Data Scientist to support the office of DoD Chief Digital and Artificial Intelligence Office (CDAO) Chief Technology Officer (CTO). CDAO CTO requires support in multiple functional areas to ensure deliverables associated with the CDAO Architecture Council, CTO Federation, and CTO Future Architecture Activities. The mission of the CDAO CTO is to accelerate the DoD's adoption of data, analytics, and AI to improve decision making across all levels of the department. The following are examples of responsibilities:\n",
        "          Support development of insider threat strategy in support of protecting CDAO technical offerings; balance short-term wins with long-term investments to progressively mature CDAO\u2019s defenses against insider threats.\n",
        "          Lead coordination of policy and strategy related to insider threats with industry partners and other DoD components.\n",
        "          Lead exploration of data sources that are relevant to measuring, identifying, and defending against insider threats.\n",
        "          Provide technical leadership in developing capabilities to detect insider threats among large user communities, leveraging combination of statistical, classical machine learning, and deep learning methods.\n",
        "          Sancorp Consulting LLC shall, in its discretion, modify or adjust the position to meet Sancorp\u2019s changing needs. This job description is not a contract and may be adjusted as deemed appropriate at Sancorp\u2019s sole discretion.\n",
        "          Sancorp Consulting, LLC, is an SDVOSB and SBA 8(a) company seeking highly motivated and qualified professionals and offer an attractive salary and benefits package that includes: Medical, Dental, life and Disability Insurance; 401K, and holidays to ensure the highest quality of life for our employees. Please visit our website for more information at www.sancorpconsulting.com.\n",
        "          Sancorp Consulting, LLC is an equal opportunity employer. At Sancorp Consulting, LLC we are committed to providing equal employment opportunities (EEO) to all employees and applicants without regard to race color, religion, sex, national origin, age, disability, or any other protected characteristic as defined by applicable law. We strive to create an inclusive and diverse workplace where everyone feels valued, respected, and supported.\"\"\"\n",
        "          '''\n",
        "result = get_completion(query=query_text, model=model, tokenizer=tokenizer)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ELGPh1LibTC"
      },
      "source": [
        "## Step 3 - Load dataset for finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKIUAjzSibTD"
      },
      "source": [
        "### Lets Load the Dataset\n",
        "\n",
        "For this tutorial, we will fine-tune Gemma-2B-IT Instruct for code generation.\n",
        "\n",
        "The dataset structure should resemble the following:\n",
        "\n",
        "```json\n",
        "{\n",
        "  ...keys,\n",
        "  \"RSD Name/Skill Tag\": \"Skill 1\", \"Skill 2\", \"Skill 3\", \"Skill 4\", ...\n",
        "  \"Skill Statement/Task\": \"Task 1\", \"Task 2\", \"Task 3\", \"Task 4\", ...\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "bb02ca238910430dbc057ab1793e47c5",
            "004da28c71144f479f8f8e10084c2b85",
            "c25205b514144041ba2bce18ca4a8d37",
            "a742cbe796be46d4bbc13bdd0e0dcf6a",
            "88f4f3acad494dffac1cab27a466da5d",
            "1be604c5ee484995a8d1332c3c9e3f90",
            "9b83b543eab3416881c97f1c84861ddd",
            "0fb97225e3df4fa09b8f8a9df96e5078",
            "b245a428d05c49e0b39ed9bfc2cf3d4c",
            "e4219ef0db8347c89bf75453fe6bd422",
            "0ca26ff91aa44b009f11ce0707d8ddef",
            "f671ea8822494d5d802337224918c781",
            "c796abc8d8a74322812541ec7524a16e",
            "3d56fbfe83d04f38adb5d1d17723a367",
            "180998cef54e4a9dbb16df3645c41586",
            "3426842a3c914272ab040dfe5f4be1d3",
            "a1cf3866b08a4ff3a7f893a7930cdbfa",
            "0642b0421c804fa19a50ef4e10e6fe44",
            "a918ca2da1d24352b64590527466d399",
            "507a75d639f94f15a59b65c6bff21b68",
            "81efccf46fab47b4bf643d2d402e9d4f",
            "1d76c96bb67643eead863c12916089ba"
          ]
        },
        "id": "-jVzUYkribTF",
        "outputId": "fd98f14d-bddc-4f16-e8ae-0a14e0de904e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb02ca238910430dbc057ab1793e47c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/518k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f671ea8822494d5d802337224918c781",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Unnamed: 0', 'Canonical URL', 'RSD Name', 'Author', 'Skill Statement', 'Category', 'Keywords', 'Standards', 'Certifications', 'Occupation Major Groups', 'Occupation Minor Groups', 'Broad Occupations', 'Detailed Occupations', 'O*Net Job Codes', 'Employers', 'Alignment Name', 'Alignment URL', 'Alignment Framework'],\n",
              "    num_rows: 932\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "# Combined OSN taxanomy dataset consists of comp, ind, and pr occupations\n",
        "dataset = load_dataset(\"Phanindra-max/osn_combined\", split=\"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0q-KkOhibTG"
      },
      "outputs": [],
      "source": [
        "# df = dataset.to_pandas()\n",
        "# df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy-FZoImibTI"
      },
      "source": [
        "Instruction Fintuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :\n",
        "1. the function generate_prompt : take the instruction and output and generate a prompt\n",
        "2. shuffle the dataset\n",
        "3. tokenizer the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2z2ucnTibTI"
      },
      "source": [
        "### Formatting the Dataset\n",
        "\n",
        "Now, let's format the dataset in the required [gemma instruction formate](https://huggingface.co/google/gemma-7b-it).\n",
        "\n",
        "> Many tutorials and blogs skip over this part, but I feel this is a really important step.\n",
        "\n",
        "```\n",
        "<start_of_turn>user What is your favorite condiment? <end_of_turn>\n",
        "<start_of_turn>model Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!<end_of_turn>\n",
        "```\n",
        "\n",
        "You can use the following code to process your dataset and create a JSONL file in the correct format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vapf6IUwibTJ"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
        "\n",
        "    :param data_point: dict: Data point\n",
        "    :return: dict: tokenzed prompt\n",
        "    \"\"\"\n",
        "    prefix_text = 'Name all the skills present in the following description in a single list. Response should have only the skills, no other information or words. Skills should be keywords, each being no more than 3 words. Below text is the Description:\\n\\n'\n",
        "    # Samples with additional context into.\n",
        "    text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"Skill Statement\"]} <end_of_turn>\\n<start_of_turn>model {data_point[\"RSD Name\"]} <end_of_turn>\"\"\"\n",
        "    return text\n",
        "\n",
        "# add the \"prompt\" column in the dataset\n",
        "text_column = [generate_prompt(data_point) for data_point in dataset]\n",
        "dataset = dataset.add_column(\"prompt\", text_column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnJE6H9NibTK"
      },
      "source": [
        "We'll need to tokenize our data so the model can understand.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b8d3aa38edec47ef9f895648efb1c0d1",
            "4232f198fce54d7d880cb1342bd2eb7a",
            "4b1a7d954d4848e1b6fe5242f8c33f08",
            "a47a55a77c6442b5acd56b126ee11b1c",
            "e25f898db5d84f07b0686171cef80bcb",
            "6312c321c06d45d38f9f119158e96641",
            "eeb786e25f184d89a4ea77a7e8de8927",
            "e8c72700e1ed4f5c92734893298c7cf1",
            "433ebfd9715b478081c38307730ae741",
            "dd4fce43fde24a9583577b016f86c643",
            "714ec816fe2b40fda0b9b8f5a10db197"
          ]
        },
        "id": "VAzSC_0OibTK",
        "outputId": "7cc1f065-aef6-42d7-da25-0b7529873338"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8d3aa38edec47ef9f895648efb1c0d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/932 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
        "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3akyCDmibTK"
      },
      "source": [
        "Split dataset into 90% for training and 10% for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLMnWZQeibTL"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpwSwQH-ibTN",
        "outputId": "248e80b2-061b-4476-833d-33267d893522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['Unnamed: 0', 'Canonical URL', 'RSD Name', 'Author', 'Skill Statement', 'Category', 'Keywords', 'Standards', 'Certifications', 'Occupation Major Groups', 'Occupation Minor Groups', 'Broad Occupations', 'Detailed Occupations', 'O*Net Job Codes', 'Employers', 'Alignment Name', 'Alignment URL', 'Alignment Framework', 'prompt', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 187\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41zGKmv7ibTP"
      },
      "source": [
        "## Step 4 - Apply Lora  \n",
        "Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg72qPywibTP"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb3EZqmeibTQ",
        "outputId": "8d2f3ddc-4104-4d13-e513-834341b26ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GemmaForCausalLM(\n",
            "  (model): GemmaModel(\n",
            "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-17): 18 x GemmaDecoderLayer(\n",
            "        (self_attn): GemmaSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "          (rotary_emb): GemmaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): GemmaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
            "          (act_fn): GELUActivation()\n",
            "        )\n",
            "        (input_layernorm): GemmaRMSNorm()\n",
            "        (post_attention_layernorm): GemmaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): GemmaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZaUTIu3ibTS"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes as bnb\n",
        "def find_all_linear_names(model):\n",
        "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "  lora_module_names = set()\n",
        "  for name, module in model.named_modules():\n",
        "    if isinstance(module, cls):\n",
        "      names = name.split('.')\n",
        "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
        "      lora_module_names.remove('lm_head')\n",
        "  return list(lora_module_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8SrOTKkibTT",
        "outputId": "354761fb-6056-4fa2-f29b-7fd255627793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['up_proj', 'v_proj', 'o_proj', 'k_proj', 'gate_proj', 'down_proj', 'q_proj']\n"
          ]
        }
      ],
      "source": [
        "modules = find_all_linear_names(model)\n",
        "print(modules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0S-wQ6qibTT"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=32,\n",
        "    target_modules=modules,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94gZCr6UibTU",
        "outputId": "158f5026-fd7e-4e71-c69c-4ccae343455d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable: 78446592 | total: 2584619008 | Percentage: 3.0351%\n"
          ]
        }
      ],
      "source": [
        "trainable, total = model.get_nb_trainable_parameters()\n",
        "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ84gpO-ibTV"
      },
      "source": [
        "## Step 5 - Run the training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbdyU60DibTV"
      },
      "source": [
        "Setting the training arguments:\n",
        "* for the reason of demo, we just ran it for few steps (100) just to showcase how to use this integration with existing tools on the HF ecosystem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfyPlH-7ibTW"
      },
      "source": [
        "### Fine-Tuning with qLora and Supervised Fine-Tuning\n",
        "\n",
        "We're ready to fine-tune our model using qLora. For this tutorial, we'll use the `SFTTrainer` from the `trl` library for supervised fine-tuning. Ensure that you've installed the `trl` library as mentioned in the prerequisites."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbizn-yfwynh"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "06c750f5d28c439ab10a5a4e8dda955a",
            "978e77644d4240129c51732fb0307205",
            "4076fec776504d5caed53d078eb9588d",
            "b4147cd357464e159702c20ee3c0d82a",
            "f5708c06eb474ce5acde9a7e813bf95b",
            "502eddeb5c88470c94a28a6fe36107cd",
            "2628477705694e94b859cd9755cbdd47",
            "e233e4a40a8d422c815a517083c12a82",
            "6d94a901d92d465b996336bf149e0ed1",
            "afe9a6b787ad4a6aa90d7079027f3aa5",
            "58d11eb7879f4505bb3a178c1fd482a4",
            "15887190a9084d63b3ed0b46574ba2a4",
            "39f29ab2d13e4bccb886327910eb6575",
            "1395b44914544d829ab19cdf23a09c92",
            "047282dbd82c4288876663fbd5153b14",
            "0e711ef6d42745f4bc7a93b952cdcec8",
            "ffaea6c395964263a15db6259242b9be",
            "5b31bde3ed544ebe926d48309300d6bd",
            "13fc5327d0fa4f34a010515a3b0a716c",
            "82d170a776c944beb1183757f603688e",
            "25a7f369424a4592b69a5e900dcf9538",
            "2097195da84c4c638c9cc41cd03578f9"
          ]
        },
        "id": "tjItUXnoibTW",
        "outputId": "0bddafd5-a79a-40c9-f039-4df011598aeb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06c750f5d28c439ab10a5a4e8dda955a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/745 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15887190a9084d63b3ed0b46574ba2a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/187 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#new code using SFTTrainer\n",
        "import transformers\n",
        "from trl import SFTTrainer\n",
        "# from transformers.generation_utils import top_k_top_p_filtering # Import the function from its new location\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    dataset_text_field=\"prompt\",\n",
        "    peft_config=lora_config,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=0.03,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        save_strategy=\"epoch\",\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU8Gc4h5ibTW"
      },
      "source": [
        "## Lets start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ls8hlQxpibTX",
        "outputId": "964965a7-92fb-4904-adf0-3725f1624139"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='54' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 54/100 03:41 < 03:16, 0.23 it/s, Epoch 0.28/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>7.958700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>7.848500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>5.536700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.155100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.201400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.725900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.082300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.762000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.481800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.507500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.564200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.373000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.295600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.276100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.298500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.081700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.209800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.201000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.936300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.907800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.838800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.939900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.737800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.954300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.728200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.875700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.799400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.906200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.826500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.926800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.803500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.777200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.802800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.739700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.791100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.898500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.821100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.783400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.939100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.788800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.862000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.677900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.808200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.730100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.833100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.655500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.854100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.738700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.560400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.598600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.577000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c15bfc7d1464>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# silence the warnings. Please re-enable for inference!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1625\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1961\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2909\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2911\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1965\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1966\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfxADB3CibTY"
      },
      "source": [
        " Share adapters on the \ud83e\udd17 Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A23aeB1zibTY"
      },
      "outputs": [],
      "source": [
        "new_model = \"100epoch-gemma-Code-Finetune-test\" #Name of the model you will be pushing to huggingface model hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6ll_p4BibTZ"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ac601ba95244458b82cf8836cfb256ff",
            "7e9411ef50d24713ba51ff5372c32b44",
            "bce6119d1b384d2ea178fa3700db4023",
            "4379512c2ce747cba72b2a4505fdbb68",
            "5eba4aba9d3a45eb9275ae3b25ef4c8d",
            "8ab5576c11a048319946677730c722a8",
            "039fbdd2b3324b1ebebae18fb6233ffa",
            "e6c8e085e89c46d88039957c61cc13ed",
            "52fbb490bb3a44c5b071c97dff2658ea",
            "5fb49231212349d4b397a393da4f1826",
            "d0c4696522134bafaaee2823581094fd"
          ]
        },
        "id": "n5Ng6IdGibTb",
        "outputId": "834c2e80-2359-4eee-e892-2ce8eaa188e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac601ba95244458b82cf8836cfb256ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
        "merged_model= merged_model.merge_and_unload()\n",
        "\n",
        "# Save the merged model\n",
        "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
        "tokenizer.save_pretrained(\"merged_model\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "726ac778dddd4408965c39074ec9d6fb",
            "700ce8277df3487b8358a0882fcd7c06",
            "8376d7ae4822444ea782eba6956fcfd1",
            "f98e31c7078e4303aff46ddc339425b0",
            "38e8c404e2154a01bc3eb0f8ec0b655a",
            "c9d1ef5d4c1146e5940b2e13ddab072b",
            "e0163f14901445d0bf217353a29df4b1",
            "dc65bc1fee184046afcf07a7b6bdf7e0",
            "dd9d957412f04861a44b669abbe5951a",
            "4ddc288a9701428c9f441fe696b0f065",
            "8dda93756d0a468887fa16f9a5691f48",
            "ce8d74f8ac294f67acb05deef9a6a115",
            "5b03cea7a4464c26aeecc2c9383dc9b8",
            "a6abf4af3d3749b1aed9adfafa21e8fc",
            "32b765a9fc9747c998011dc4201dd212",
            "6e49f58448c94a97b8c526aeade5f9fe",
            "84d112d0e1854fb58ac1c25b2b2dee04",
            "6d18dd72fe4e4ac598e42b6ef3923366",
            "e3e1efc609f6408e924bdcde3b4f829b",
            "fda54ae83a5644239d60ec8b61f8bf98",
            "87f8ac706f114391afd6ba16bbfbb71d",
            "5791a34c8a854a05aee36d1cc3ac926f",
            "15f2c440914e4b35ac430f21a386a5ff",
            "4517328ee54d4894a1dbc1563a71468e",
            "4a5640ce384f4b35b8aacb842b407fa2",
            "3f5ce8de08984e059e4ab11784d15cce",
            "a208606aadc84469b647e37a80101e5f",
            "dfe6d6f62c454ae585f147cd2bbc268e",
            "625e5746a5084a978c7d7fdefcdc5282",
            "940f9f97e1b24018ab97eec95d50501f",
            "254fb63a66594f90873230aa63382105",
            "2ac158fb217d467895009e1546e14561",
            "72b786abe4a8443281540f7a7aa8d05d",
            "ba982791de14425ca8ec83e6fbf18cb8",
            "7d280792378546abb3653be766dad8e0",
            "93797c0d45c440d9926a1150a2873c70",
            "b4c13068d8ce41c3bd9d4e1812b37e6f",
            "21f7f2f3c78546c08970269d03f321aa",
            "d79815cb365a4091a23cc0c1e70fab62",
            "1aaede57be8f4bce8490c22fb34baf7c",
            "74c4a0a9a13040669811142018abd1af",
            "5be2b080704c43a6964f4c3e5499bd03",
            "60edd42298dc4223b260099c6f29b206",
            "7203a4d61f67489baca9c7caf9a72272",
            "90ae8617641b45219e74cdcc32b47194",
            "7d895588d9c84b96b1aefa64d4d712a2",
            "33ed0dd3eb96457f9cc9db3f7d6fdb39",
            "5f28415f288a4804b7cfe04d76d3a353",
            "7aa457d506094223b7a5b8e8836b2487",
            "bba8cd616f3e4276b143d98432e8b7fc",
            "9e2eeae0b6ca49728d50a4d76c984a00",
            "35704d5837074e609b8ead9e060317f9",
            "c1274413b868405691207e8bd6becc14",
            "8a449e5a03bd41ae8efbecf26202bc45",
            "46374779fec0492ba943a0e6bd176c1e",
            "07e9b5e98b5b459c961037be29593a6a",
            "4d64db02d3a94584883a25c528aed74f",
            "de1abe7ee0d545f98230ed3bd8dc2be7",
            "a12a157505084682ad72a34b82fb92c7",
            "2ce9b257ff3a4cfcac16853e81aebc1a",
            "32c598265127459f94790fcae04e012a",
            "bcee05e87a7349d1a30efe3e87810e98",
            "2b16177583e54d5ba85a216841baed75",
            "cdb1d23cea314b89be2dfdcc9aa89bf1",
            "70bcae6000f14918861b3ab26a466735",
            "34feae71ecc34b2b869386bef377ed60",
            "471e1aadbfe54b6e95abb062509efc00",
            "2d1727fda2f447ee84c3cf257427a30e",
            "f9dff78987374190a69a10b493b90ffa",
            "37f1c05cefe14408b520ad5db12fe3d4",
            "45845ebd059e407596fb6f72f28f593d",
            "dc4ecdd47b184f7eae7023a55cadfdc8",
            "4f812ef1ca304d22a54e729138f1f184",
            "05661886f20146c092b0181a36540f29",
            "398b654f9ccb4e65a27fb585933d1a9f",
            "739c857604274c91ac846734237d36d5",
            "d873b405d79249ed8cf93edd84b5aad1"
          ]
        },
        "id": "ZqK0UVSXibTc",
        "outputId": "1375fc69-a2c1-4e91-b6c7-331a1df818aa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "726ac778dddd4408965c39074ec9d6fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce8d74f8ac294f67acb05deef9a6a115",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15f2c440914e4b35ac430f21a386a5ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba982791de14425ca8ec83e6fbf18cb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90ae8617641b45219e74cdcc32b47194",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07e9b5e98b5b459c961037be29593a6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "471e1aadbfe54b6e95abb062509efc00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Phanindra-max/100epoch-gemma-Code-Finetune-test/commit/0355471197c5f50403406bb6ecc427c4aec64499', commit_message='Upload tokenizer', commit_description='', oid='0355471197c5f50403406bb6ecc427c4aec64499', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Push the model and tokenizer to the Hugging Face Model Hub\n",
        "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoANkwTZOiav"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--7gvP0kibTe"
      },
      "source": [
        "## Test out Finetuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPdqbvUBSOwB"
      },
      "outputs": [],
      "source": [
        "text = '''As a Research Engineer at Whissle LLC, you will play a pivotal role in bringing our cutting-edge research to life. Your work will involve implementing and experimenting with the latest research techniques, and developing tools and infrastructure that streamline the transition of research into viable products.\n",
        "\n",
        "\n",
        "Key Responsibilities:\n",
        "\n",
        "    Experiment with and adopt the latest research techniques in AI and machine learning, open-source and our own research.\n",
        "    Develop, maintain, and enhance benchmarks for evaluating AI performance.\n",
        "    Implement and continuously refine our agent architecture to improve functionality and efficiency.\n",
        "    Contribute to the creation of a seamless experimental framework, supporting the integration of research findings into product development.\n",
        "\n",
        "\n",
        "Qualifications:\n",
        "\n",
        "    Solid background in software engineering and ML development, with expertise in optimized deployment and inference of multi-modal AI solutions (video, audio, speech LLMs).\n",
        "    Proficiency in Python and C++, with experience in machine learning frameworks and dependencies (e.g., torch, tensorrt, cuda).\n",
        "    Experience with creating dev tools, which include hosting on-demand micro-services.\n",
        "    Strong skills in version control, code reviews, and CI/CD practices.\n",
        "    Familiarity with automated integration and deployment environments.\n",
        "    Exceptional problem-solving abilities and innovative thinking.\n",
        "    Excellent teamwork and communication skills.\n",
        "\n",
        "\n",
        "Preferred Experience:\n",
        "\n",
        "    Previous role as a research engineer at leading general AI companies like Netflix, Google AI, Anthropic, or OpenAI or in a leading AI research organization.\n",
        "    Extensive exposure to cutting-edge AI research, especially in multi-modal AI technologies.\n",
        "    Track record of contributing to AI advancements through publications, patents, or open-source projects.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXAT1Eg6ibTe",
        "outputId": "6dcb7a21-2f56-4733-a7e9-e4596e2d3192"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result from original model:\n",
            " \n",
            "  user\n",
            "  Name all the skills present in the following description in a single list. Response should have only the skills, no other information or words. Skills should be keywords, each being no more than 3 words.\n",
            "  Below text is the Description:\n",
            "\n",
            "  As a Research Engineer at Whissle LLC, you will play a pivotal role in bringing our cutting-edge research to life. Your work will involve implementing and experimenting with the latest research techniques, and developing tools and infrastructure that streamline the transition of research into viable products.\n",
            "\n",
            "\n",
            "Key Responsibilities:\n",
            "\n",
            "    Experiment with and adopt the latest research techniques in AI and machine learning, open-source and our own research.\n",
            "    Develop, maintain, and enhance benchmarks for evaluating AI performance.\n",
            "    Implement and continuously refine our agent architecture to improve functionality and efficiency.\n",
            "    Contribute to the creation of a seamless experimental framework, supporting the integration of research findings into product development.\n",
            "\n",
            "\n",
            "Qualifications:\n",
            "\n",
            "    Solid background in software engineering and ML development, with expertise in optimized deployment and inference of multi-modal AI solutions (video, audio, speech LLMs).\n",
            "    Proficiency in Python and C++, with experience in machine learning frameworks and dependencies (e.g., torch, tensorrt, cuda).\n",
            "    Experience with creating dev tools, which include hosting on-demand micro-services.\n",
            "    Strong skills in version control, code reviews, and CI/CD practices.\n",
            "    Familiarity with automated integration and deployment environments.\n",
            "    Exceptional problem-solving abilities and innovative thinking.\n",
            "    Excellent teamwork and communication skills.\n",
            "\n",
            "\n",
            "Preferred Experience:\n",
            "\n",
            "    Previous role as a research engineer at leading general AI companies like Netflix, Google AI, Anthropic, or OpenAI or in a leading AI research organization.\n",
            "    Extensive exposure to cutting-edge AI research, especially in multi-modal AI technologies.\n",
            "    Track record of contributing to AI advancements through publications, patents, or open-source projects.\n",
            "\n",
            "  \n",
            "model\n",
            " Emerging AI Technology Experimentation \n",
            "model Explore Latest AI Research \n",
            "Scopri the latest AI tools and techniques \n",
            " Develop Benchmarks \n",
            " Experiment with AI \n",
            " Contribute to Artificial Intelligence Development \n",
            " Participate in AI Collaboration \n",
            " Embrace Collaboration \n",
            " Promote AI \n",
            " Automate Products \n",
            " model Emerging AI Technologies \n",
            " vorrei AI Collaboration  Pijp with AI \n",
            " Model Development \n",
            " Model Deployment \n",
            " Research \n",
            " Team Collaboration \n",
            " Comprehensive AI Design \n",
            " AI Innovation \n",
            " Artificial Intelligence \n",
            " Continuous AI Monitoring \n",
            " AI Products \n",
            " Maintain AI Research \n",
            " Artificial Intelligence Collaborations \n",
            " Maintain Model Training \n",
            " Optimize AI Tools \n",
            " Artificial Intelligence Automation \n",
            " Continuous Delivery \n",
            " AI Metrics \n",
            " Artificial Intelligence Deployment \n",
            " \n",
            " Maintain AI Tools \n",
            " Maintain Model Training \n",
            " Continuous AI Monitoring \n",
            " Maintain AI Systems \n",
            " AI Model Continuous Monitoring \n",
            " Maintain AI Applications \n",
            " Artificial Intelligence \n",
            " Artificial Intelligence \n",
            " Embrace Collaboration \n",
            " Embrace Continuous Improvement \n",
            " Promote AI \n",
            " Track AI metrics \n",
            " Optimize AI Tools \n",
            " Enhance AI Tools  Dage \n",
            " Artificial Intelligence Development \n",
            " \n",
            " Collaboration \n",
            " Continuous Integration  Godt \n",
            " Continuous Monitoring \n",
            " Machine Learning \n",
            " Reinforcement Learning \n",
            " Computer Aided Design  Pasa del software Specifiche de dise\u00f1o de software \n",
            " Product Development \n",
            " Cloud Computing \n",
            " Programming \n",
            " Software Engineering \n",
            " Algorithm Design \n",
            " Reinforcement Learning \n",
            " Automated Testing \n",
            " Continuously Implement AI  perfeitamente \n",
            " Full Stack AI \n",
            " Artificial Intelligence \n",
            " Maintain AI Frameworks \n",
            " Automate System \n",
            " Artificial Intelligence \n",
            " Continuous Improvements \n",
            " Continuous \n",
            " AI Collaborative Design \n",
            " AI Data Acquisition LEGGI model Emerging AI Technologies \n",
            " Develop AI Applications  \n",
            " Continuously Implement AI  perfeitamente \n",
            " Maintain AI Frameworks \n",
            " Continuous AI Monitoring \n",
            " Detect Potential  pungkasnya  Pyrene \n",
            " Identify Metrics \n",
            " Collaborate with AI  Pasa \n",
            " Participate in AI  \ud83d\ude0f <unused8>\n",
            " Contribute to Continued Research \u0015\n",
            " Maintain AI Research \n",
            " Track AI Metrics  Dom\u00ednguez \n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_Tj-p4mSVCx",
        "outputId": "048b45b2-12b4-4958-8b58-0f443c90344d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result from fine-tuned and merged model:\n",
            " \n",
            "  user\n",
            "  Name all the skills present in the following description in a single list. Response should have only the skills, no other information or words. Skills should be keywords, each being no more than 3 words.\n",
            "  Below text is the Description:\n",
            "\n",
            "  As a Research Engineer at Whissle LLC, you will play a pivotal role in bringing our cutting-edge research to life. Your work will involve implementing and experimenting with the latest research techniques, and developing tools and infrastructure that streamline the transition of research into viable products.\n",
            "\n",
            "\n",
            "Key Responsibilities:\n",
            "\n",
            "    Experiment with and adopt the latest research techniques in AI and machine learning, open-source and our own research.\n",
            "    Develop, maintain, and enhance benchmarks for evaluating AI performance.\n",
            "    Implement and continuously refine our agent architecture to improve functionality and efficiency.\n",
            "    Contribute to the creation of a seamless experimental framework, supporting the integration of research findings into product development.\n",
            "\n",
            "\n",
            "Qualifications:\n",
            "\n",
            "    Solid background in software engineering and ML development, with expertise in optimized deployment and inference of multi-modal AI solutions (video, audio, speech LLMs).\n",
            "    Proficiency in Python and C++, with experience in machine learning frameworks and dependencies (e.g., torch, tensorrt, cuda).\n",
            "    Experience with creating dev tools, which include hosting on-demand micro-services.\n",
            "    Strong skills in version control, code reviews, and CI/CD practices.\n",
            "    Familiarity with automated integration and deployment environments.\n",
            "    Exceptional problem-solving abilities and innovative thinking.\n",
            "    Excellent teamwork and communication skills.\n",
            "\n",
            "\n",
            "Preferred Experience:\n",
            "\n",
            "    Previous role as a research engineer at leading general AI companies like Netflix, Google AI, Anthropic, or OpenAI or in a leading AI research organization.\n",
            "    Extensive exposure to cutting-edge AI research, especially in multi-modal AI technologies.\n",
            "    Track record of contributing to AI advancements through publications, patents, or open-source projects.\n",
            "\n",
            "  \n",
            "model\n",
            " Research Implementations \n",
            "model Software Implementation \n",
            " Software Engineering \n",
            " AI Research Participation \n",
            " Machine Learning Frameworks \n",
            " Machine Learning Algorithm Design \n",
            " Code Review \n",
            " Version Control \n",
            " Software Design \n",
            " CI/CD \n",
            " Troubleshooting \n",
            " Product Design \n",
            " Prototype Implementation \n",
            " AI Research Contribution \n",
            " Software Engineering \n",
            " Machine Learning \n",
            " Collaboration \n",
            " Communication \n",
            " Multi-modal AI Implementation \n",
            " Public Domain Tools \n",
            " Open-source Contribution \n",
            " \n",
            " Monitoring  \n",
            "  \n",
            " Programming \n",
            " Research Collaboration \n",
            " \n",
            " Machine Learning Experiments \n",
            " Experiment Implementation \n",
            " \n",
            " Research Frameworks \n",
            " AI Software \n",
            " Software Engineering \n",
            " Model Design \n",
            " Model Development \n",
            " Data Integration \n",
            " \n",
            " Open-source Software Collaboration \n",
            " \n",
            " AI Application Research  \n",
            " \n",
            " \n",
            " Collaborative Development \n",
            "  \n",
            " Software Engineering C\u00e1pacities \n",
            " \n",
            " Software Engineering  Pr\u00e6cises  d\u00e9bris \n",
            " \n",
            " Software Engineering  Wi\u0119ks \n",
            " Software Engineering  asyncio \n",
            " \n",
            " Feature-Driven Development \n",
            " Debugging \n",
            " Technical Implementation \n",
            " Continuous Integration  b\u00e2ton \n",
            " \n",
            " Python \n",
            " Algorithm Design \n",
            " Open-source Projects Participation \n",
            " Machine Learning Libraries \n",
            " Software Engineering Practices  Dom\u00ednguez \n",
            " Artificial Intelligence \n",
            " AI Research  Godt \n",
            " \n",
            " Algorithm Research  Pr\u00e6entes \n",
            " \n",
            " Machine Learning  pavillon \u0423\u0434\u0430\u0447\u0438 \n",
            " \n",
            " Research Collaboration \n",
            " \n",
            " AI-Powered Integrations \n",
            " \n",
            " Collaborate with AI Professionals \n",
            " model Research Assistant \n",
            " Machine Learning \n",
            " Programming  AssemblyCulture AssemblyCulture AssemblyCulture AssemblyCulture \n",
            " \n",
            " Multi-modal AI Application Development \n",
            " Software Engineering \n",
            " Collaborative Application Development \n",
            " Continuous Integration \n",
            " Model Development \n",
            " \n",
            " System Design \n",
            " Algorithm Research \n",
            " Research Collaboration \u0423\u0434\u0430\u0447\u0438 \n",
            " Software Engineering C\u00e1pacities \n",
            " Continuous Improvement  \n",
            " Collaborative Application \n",
            " \n",
            " Modelling \n",
            " Code Review \n",
            " Engineering \n",
            " AI-Powered Integrations \n",
            " \n",
            " Algorithm Design \n",
            " model Contribute to AI Refinement \n",
            "  \n",
            " Contribute to AIO Refinement \n",
            " \n",
            " Research Research  \n",
            "  Godt \n",
            " Contribute to Experimentation \n",
            "  \n",
            " Contribute to Model Refinement \n",
            " \n",
            " Optimize Applications  attuale \n",
            " \n",
            " Ensure AI Systems Security \n",
            " \n",
            " Maintain Research Data \n",
            "  Pasa  thuyper \n",
            " Scopri  Model \n",
            " \n",
            " Implement New Research \n",
            "   \n",
            " Support AI  pavillon \n",
            "  \n",
            " Manage AI Datasets \n",
            " \n",
            " Contribute to Model Updates \n",
            " Specifiche \n",
            " Participate in AI Efforts \n",
            "  \n",
            " Algorithm  Boven \n",
            " Modelling  maquillage \n",
            " model Continuous Integrations \n",
            " \n",
            " Feature Identification \n",
            " Prototype Refinement \n",
            " Application Research \n",
            " Continuous Refinement \n",
            " Open-source Contribution \n",
            " Research Collaboration \n",
            " Research  \n",
            " Specifiche  Pijamas  pungkasnya \n",
            " Contribute model Software Engineering \n",
            " \n",
            " Software Engineering Practices  Dom\u00ednguez  \n",
            "  patrie  coll\u00e8ge \n",
            " Optimize Deployment \n",
            " Code Review  Dage \n",
            " Continuous Integrations anoke  \n",
            " Collaboration  d\u00e9bris \n",
            "  Guimar\u00e3es  d\u00e9bris  couver \n",
            "    perfeitamente \n",
            " Continuous Integrations anoke \n",
            "  Mej\u00eda  Dage  rechange  Dom\u00ednguez  maquillage  pungkasnya\n",
            " Specifiche  Pijamas \n",
            "  rechange Project  Pijamas \n",
            "  Dage  pavillon \n",
            " Software Engineering  habang   Camargo \n",
            "  Guimar\u00e3es  Dage  maquillage \n",
            "  Camargo, Sophia  Pijamas  Khart  coll\u00e8ge   \n",
            "  Dom\u00ednguez  Dage  Pijamas \n",
            "  Dom\u00ednguez, Sophia  mitsubishi \n",
            "  Guimar\u00e3es  Dage  lumine \n",
            "  Mej\u00eda, Sophia  Dage  Alha  Godt  \n",
            "  Mej\u00eda, Sophia  Pijamas \n",
            "  Mej\u00eda, Sophia  Dage  Dom\u00ednguez \n",
            "  M\u00e9ndez, Sophia  Pijamas \n",
            "  Mej\u00eda, Sophia  Dage  Camargo  Rom\u00e2    C\u00e1rdenas\n"
          ]
        }
      ],
      "source": [
        "# query the fine-tuned model to compare the outputs\n",
        "result = get_completion(query=text, model=merged_model, tokenizer=tokenizer)\n",
        "print(\"Result from fine-tuned and merged model:\\n\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjJ4-NmsVxVu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZNuTFDhYAb9",
        "outputId": "d5a4b657-cc28-484a-dfa0-525489ca0a06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result from original model:\n",
            " \n",
            "  user\n",
            "  Name all the skills present in the following description in a single list. Response should have only the skills, no other information or words. Skills should be keywords, each being no more than 3 words.\n",
            "  Below text is the Description:\n",
            "\n",
            "  As a Research Engineer at Whissle LLC, you will play a pivotal role in bringing our cutting-edge research to life. Your work will involve implementing and experimenting with the latest research techniques, and developing tools and infrastructure that streamline the transition of research into viable products.\n",
            "\n",
            "\n",
            "Key Responsibilities:\n",
            "\n",
            "    Experiment with and adopt the latest research techniques in AI and machine learning, open-source and our own research.\n",
            "    Develop, maintain, and enhance benchmarks for evaluating AI performance.\n",
            "    Implement and continuously refine our agent architecture to improve functionality and efficiency.\n",
            "    Contribute to the creation of a seamless experimental framework, supporting the integration of research findings into product development.\n",
            "\n",
            "\n",
            "Qualifications:\n",
            "\n",
            "    Solid background in software engineering and ML development, with expertise in optimized deployment and inference of multi-modal AI solutions (video, audio, speech LLMs).\n",
            "    Proficiency in Python and C++, with experience in machine learning frameworks and dependencies (e.g., torch, tensorrt, cuda).\n",
            "    Experience with creating dev tools, which include hosting on-demand micro-services.\n",
            "    Strong skills in version control, code reviews, and CI/CD practices.\n",
            "    Familiarity with automated integration and deployment environments.\n",
            "    Exceptional problem-solving abilities and innovative thinking.\n",
            "    Excellent teamwork and communication skills.\n",
            "\n",
            "\n",
            "Preferred Experience:\n",
            "\n",
            "    Previous role as a research engineer at leading general AI companies like Netflix, Google AI, Anthropic, or OpenAI or in a leading AI research organization.\n",
            "    Extensive exposure to cutting-edge AI research, especially in multi-modal AI technologies.\n",
            "    Track record of contributing to AI advancements through publications, patents, or open-source projects.\n",
            "\n",
            "  \n",
            "model\n",
            " Research Techniques in AI and Machine Learning \n",
            "model Artificial Intelligence Research \n",
            "model Machine Learning Frameworks \n",
            "model Software Engineering \n",
            "model Product Development \n",
            " vorrei Framework Synthesis \n",
            "model AI Metrics and Benchmark \n",
            "Dzi\u0119kuj\u0119 model Software Engineering \n",
            "model Automated Testing and Instrumentation \n",
            "model Code Reviews \n",
            "model Software and Infrastructure Development \n",
            "model Reinforcement Learning \n",
            "model Domain-specific AI \n",
            " ficando with Artificial Intelligence \n",
            " vorrei Software Engineering \n",
            " voglio AI Research \n",
            " echipments \n",
            " Automated Software \n",
            " Continuous Integration \n",
            " Model-as-a-Service \n",
            " Software Development \n",
            " Research \n",
            " Python Machine Learning \n",
            " Existing Scripting and Tools \n",
            " AI Research Collaboration \n",
            " Machine Learning Applications  \n",
            " Machine Learning Research \n",
            " Jupyter \n",
            " Reinforcement Learning  d\u00e9bris \n",
            " Open-source AI Tools  \n",
            " model Model Engineering  \n",
            " Continuous Delivery \n",
            " AI Research Metrics \n",
            " Domain-specific AI \n",
            " Reinforcement Learning \n",
            " Automated Testing \n",
            " Regular Research \n",
            " Open-source Project \n",
            " Artificial Intelligence Research \n",
            " Jupyter  umana \n",
            " Jupyter  \ud83d\ude18 \n",
            " Artificial Intelligence Research \n",
            " Machine Learning \n"
          ]
        }
      ],
      "source": [
        "# clear the GPU cache to prevent hardware bottlneck\n",
        "torch.cuda.empty_cache()\n",
        "result = get_completion(query=text, model=model, tokenizer=tokenizer)\n",
        "print(\"Result from original model:\\n\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMgEeiPLczoU"
      },
      "source": [
        "From above, it can be observed that the original model performs better than the fine-tuned version.\n",
        " - Could be because of me stopping the training function at 50 epochs.\n",
        " - Irrespective of the model being fine-tuned or not, the results from 2B model are clearly bad when compared to the output of the 7B base model.\n",
        " - Non-english words, Emojis and Any other special characters are NOT expected in the output. TODO: need to experiment trying a couple variations of the query instruct. 7B base model never gave Non-english words or Speacial charcters.\n",
        " - The response shouldn't be huge either. Ideally 5-10 skills seems nice to me. TODO: try a few experiments by changing the query to have a limit on the no.of extracted skill keywords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNyxRQ5mqwGi"
      },
      "source": [
        "## Usage of the LLM\n",
        "\n",
        "- With the fine-tuned model saved on HuggingFace, I tried running a simple query.\n",
        "- One immediate problem I see is that the model is only RAM, no GPU usage recorded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "jFvcebDqbPEg",
        "outputId": "01794c01-3a6f-4cc3-cdfc-0701f2d3361b"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3b0fc49883c5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use a pipeline as a high-level helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m messages = [\n\u001b[1;32m      5\u001b[0m     {\"role\": \"user\", \"content\": '''Name all the skills present in the following description in a single list. Response should have only the skills, no other information or words. Skills should be keywords, each being no more than 3 words. Below text is the Description: As a Research Engineer at Whissle LLC, you will play a pivotal role in bringing our cutting-edge research to life. Your work will involve implementing and experimenting with the latest research techniques, and developing tools and infrastructure that streamline the transition of research into viable products.\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconversational\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConversationalPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdepth_estimation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDepthEstimationPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdocument_question_answering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocumentQuestionAnsweringPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureExtractionPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfill_mask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFillMaskPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/document_question_answering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_pipeline_init_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquestion_answering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselect_starts_ends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSquadExample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSquadFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquad_convert_examples_to_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelcard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdefault_data_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglue_compute_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxnli_compute_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .processors import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mDataProcessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_sklearn_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspearmanr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatthews_corrcoef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    606\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 608\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_mstats_basic\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_distn_infrastructure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrv_discrete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrv_continuous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrv_frozen\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_continuous_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_discrete_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_continuous_distns.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m \u001b[0manglit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manglit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'anglit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, seed)\u001b[0m\n\u001b[1;32m   1843\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m                 \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistcont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m_construct_doc\u001b[0;34m(self, docdict, shapes_vals)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_construct_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes_vals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;34m\"\"\"Construct the instance docstring with string substitutions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m         \u001b[0mtempdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m         \u001b[0mtempdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'distname'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0mtempdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shapes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshapes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": '''As a Research Engineer at Whissle LLC, you will play a pivotal role in bringing our cutting-edge research to life. Your work will involve implementing and experimenting with the latest research techniques, and developing tools and infrastructure that streamline the transition of research into viable products.\n",
        "\n",
        "\n",
        "Key Responsibilities:\n",
        "\n",
        "    Experiment with and adopt the latest research techniques in AI and machine learning, open-source and our own research.\n",
        "    Develop, maintain, and enhance benchmarks for evaluating AI performance.\n",
        "    Implement and continuously refine our agent architecture to improve functionality and efficiency.\n",
        "    Contribute to the creation of a seamless experimental framework, supporting the integration of research findings into product development.\n",
        "\n",
        "\n",
        "Qualifications:\n",
        "\n",
        "    Solid background in software engineering and ML development, with expertise in optimized deployment and inference of multi-modal AI solutions (video, audio, speech LLMs).\n",
        "    Proficiency in Python and C++, with experience in machine learning frameworks and dependencies (e.g., torch, tensorrt, cuda).\n",
        "    Experience with creating dev tools, which include hosting on-demand micro-services.\n",
        "    Strong skills in version control, code reviews, and CI/CD practices.\n",
        "    Familiarity with automated integration and deployment environments.\n",
        "    Exceptional problem-solving abilities and innovative thinking.\n",
        "    Excellent teamwork and communication skills.\n",
        "\n",
        "\n",
        "Preferred Experience:\n",
        "\n",
        "    Previous role as a research engineer at leading general AI companies like Netflix, Google AI, Anthropic, or OpenAI or in a leading AI research organization.\n",
        "    Extensive exposure to cutting-edge AI research, especially in multi-modal AI technologies.\n",
        "    Track record of contributing to AI advancements through publications, patents, or open-source projects.\n",
        "'''},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"Phanindra-max/100epoch-gemma-Code-Finetune-test\", max_new_tokens=400)\n",
        "pipe(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "5slH3Q_w2pYh",
        "outputId": "96e50c13-ef1f-41b8-cede-80395024763e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.39.1\n",
            "    Uninstalling transformers-4.39.1:\n",
            "      Successfully uninstalled transformers-4.39.1\n",
            "Successfully installed tokenizers-0.19.1 transformers-4.41.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f33e902b4f2747b096e1ec988c816a07",
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install transformers -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706,
          "referenced_widgets": [
            "212b73b73f58455599400be41dda506f",
            "221f2bb49d894c3f9bf3014178ea19da",
            "6825662021ac4795aa07e8bba529c911",
            "2c936648fa8f43e8896aeca2071bbbf8",
            "0d2cd60e007e4c1fbef71f5d27637e9f",
            "9f524003a4ad4af38b86a4ccd88196b2",
            "db26ef9fd68f4b95b7c4e0ae2c636be0",
            "f9387d4ec9ce496aa9eccf812320432f",
            "4b2d2beebca947599e8533e2437af8b5",
            "b467f719b64f47be8c099c3cf16b05bc",
            "7c238607e73a48648da9bbad1c2d185e"
          ]
        },
        "id": "ivngy2JPwW_F",
        "outputId": "3400b607-27fe-405a-b6b9-e5c332fe2376"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "212b73b73f58455599400be41dda506f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "GemmaForCausalLM(\n",
              "  (model): GemmaModel(\n",
              "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-17): 18 x GemmaDecoderLayer(\n",
              "        (self_attn): GemmaSdpaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (rotary_emb): GemmaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): GemmaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
              "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
              "          (act_fn): PytorchGELUTanh()\n",
              "        )\n",
              "        (input_layernorm): GemmaRMSNorm()\n",
              "        (post_attention_layernorm): GemmaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): GemmaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Phanindra-max/100epoch-gemma-Code-Finetune-test\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Phanindra-max/100epoch-gemma-Code-Finetune-test\")\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-6hH7Y9xdLR",
        "outputId": "7eb1de04-5311-4c7d-889f-c208680135cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  user\n",
            "  Name all the skills present in the following description in a single list. Response should be in English and have only the skills, no other information or words. Skills should be keywords, each being no more than 3 words.\n",
            "  Below text is the Description:\n",
            "\n",
            "  As a Research Engineer at Whissle LLC, you will play a pivotal role in bringing our cutting-edge research to life. Your work will involve implementing and experimenting with the latest research techniques, and developing tools and infrastructure that streamline the transition of research into viable products.\n",
            "\n",
            "\n",
            "Key Responsibilities:\n",
            "\n",
            "    Experiment with and adopt the latest research techniques in AI and machine learning, open-source and our own research.\n",
            "    Develop, maintain, and enhance benchmarks for evaluating AI performance.\n",
            "    Implement and continuously refine our agent architecture to improve functionality and efficiency.\n",
            "    Contribute to the creation of a seamless experimental framework, supporting the integration of research findings into product development.\n",
            "\n",
            "\n",
            "Qualifications:\n",
            "\n",
            "    Solid background in software engineering and ML development, with expertise in optimized deployment and inference of multi-modal AI solutions (video, audio, speech LLMs).\n",
            "    Proficiency in Python and C++, with experience in machine learning frameworks and dependencies (e.g., torch, tensorrt, cuda).\n",
            "    Experience with creating dev tools, which include hosting on-demand micro-services.\n",
            "    Strong skills in version control, code reviews, and CI/CD practices.\n",
            "    Familiarity with automated integration and deployment environments.\n",
            "    Exceptional problem-solving abilities and innovative thinking.\n",
            "    Excellent teamwork and communication skills.\n",
            "\n",
            "\n",
            "Preferred Experience:\n",
            "\n",
            "    Previous role as a research engineer at leading general AI companies like Netflix, Google AI, Anthropic, or OpenAI or in a leading AI research organization.\n",
            "    Extensive exposure to cutting-edge AI research, especially in multi-modal AI technologies.\n",
            "    Track record of contributing to AI advancements through publications, patents, or open-source projects.\n",
            "\n",
            "  \n",
            "model\n",
            " AI Research Engineer Role Implementation \n",
            "model Research Engineering Project Implementation \n",
            "model Contribute to the Advancement of AI \n",
            "model AI Research Techniques Implementation \n",
            "model Research Metrics Creation \n",
            "\n",
            " Troubleshooting \n",
            " Model Evaluation \n",
            " AI Research Ethics and Responsible Practice \n",
            " Automate Experimental Frameworks \n",
            " Contribute to Product Development \n",
            " Contribute to Algorithm Deployment \n",
            " Design and Implement AI-enabled software \n",
            " Organize AI Research Projects \n",
            " Contribute to AI Research Funding Applications \n",
            " model Continuous Improvement \n",
            " Implement Continuous Integration \n",
            " Operate a Research Engineering Platform \n",
            " Contribute to AI Products, Products \u0423\u0434\u0430\u0447\u0438 \n",
            " Deliver AI Solutions \n",
            " Collaborate on Research Teams \n",
            " Communicate Effectively \n",
            " Manage AI Research Projects \n"
          ]
        }
      ],
      "source": [
        "result = get_completion(query=text, model=model, tokenizer=tokenizer)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTZfXQ7y3_ul"
      },
      "source": [
        "### Observation\n",
        "\n",
        "- The above implementation utilizes GPU.\n",
        "- The results are a bit inaccurate with some skills some responses being in different languages.\n",
        "- Even if the fine-tune training prompts and individual query both mention clearly to use English only, the model fails to keep that in context.\n",
        "- The performance increased a lot, takes only about a few seconds to execute a prompt.\n",
        "- The results are also not too consistent, sometimes the model extracts high-quality skill keywords. Sometimes it fails to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWre61onx7C0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6woKOcXM5sji"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}